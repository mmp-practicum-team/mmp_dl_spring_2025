{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать посимвольную RNN на текстах Шекспира"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-18 10:52:20--  https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/refs/heads/master/datasets/shakespeare.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6347705 (6.1M) [text/plain]\n",
      "Saving to: ‘shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   6.05M  4.82MB/s    in 1.3s    \n",
      "\n",
      "2025-03-18 10:52:22 (4.82 MB/s) - ‘shakespeare.txt’ saved [6347705/6347705]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/refs/heads/master/datasets/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,\\'?!]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_preprocessed(path: str) -> Path:\n",
    "    \"\"\"\n",
    "    Preprocess text and save it to disk.\n",
    "\n",
    "    Returns:\n",
    "        path to saved text\n",
    "    \"\"\"\n",
    "    with Path(path).open() as file:\n",
    "        text = file.read()\n",
    "    processed = preprocess(text)\n",
    "    res = Path(\"processed.txt\")\n",
    "    with res.open(\"w\") as file:\n",
    "        file.write(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = save_preprocessed(\"shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "def train_test_split(path: str, test_size: float = 0.1) -> tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Split dataset to train and test.\n",
    "    \n",
    "    Returns:\n",
    "        train_path and test_path\n",
    "    \"\"\"\n",
    "    with Path(path).open() as file:\n",
    "        text = file.read()\n",
    "    test_num_chars = floor(len(text) * test_size)\n",
    "    test_text = text[-test_num_chars:]\n",
    "    train_text = text[:-test_num_chars]\n",
    "    train_path = Path(\"train.txt\")\n",
    "    with train_path.open(\"w\") as file:\n",
    "        file.write(train_text)\n",
    "    test_path = Path(\"test.txt\")\n",
    "    with test_path.open(\"w\") as file:\n",
    "        file.write(test_text)\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = train_test_split(\"processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.id2token = list(string.ascii_letters + string.digits + \".,\\'?! \\n\")\n",
    "        self.token2id = {char: token_id for token_id, char in enumerate(self.id2token)}\n",
    "    \n",
    "    def encode(self, txt: str) -> list[int]:\n",
    "        return [self.token2id[tok] for tok in txt]\n",
    "    \n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        return [self.id2token[tok_id] for tok_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ShakespeareTexts(Dataset):\n",
    "    def __init__(self, path: str, seq_length: int) -> None:\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = self._load_sequences(path)\n",
    "\n",
    "        self.tokenizer = CharTokenizer()\n",
    "    \n",
    "    def _load_sequences(self, path: str) -> list[str]:\n",
    "        res = []\n",
    "        with Path(path).open() as file:\n",
    "            while True:\n",
    "                sequence = file.read(self.seq_length)\n",
    "                if not sequence:\n",
    "                    break\n",
    "                res.append(sequence)\n",
    "        if len(res[-1]) != self.seq_length:\n",
    "            res.pop()\n",
    "        return res\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> torch.IntTensor:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tokenized sequence\n",
    "        \"\"\"\n",
    "        seq = self.sequences[index]\n",
    "        token_ids = self.tokenizer.encode(seq)\n",
    "        return torch.IntTensor(token_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177198, 19688)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ShakespeareTexts(train_path, seq_length=32)\n",
    "test_dataset = ShakespeareTexts(test_path, seq_length=32)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([53, 58, 52, 55, 68, 68, 26, 37, 37, 44, 67, 48, 30, 37, 37, 67, 45, 33,\n",
      "        26, 45, 67, 30, 39, 29, 44, 67, 48, 30, 37, 37, 68, 68],\n",
      "       dtype=torch.int32)\n",
      "['1', '6', '0', '3', '\\n', '\\n', 'A', 'L', 'L', 'S', ' ', 'W', 'E', 'L', 'L', ' ', 'T', 'H', 'A', 'T', ' ', 'E', 'N', 'D', 'S', ' ', 'W', 'E', 'L', 'L', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]),\n",
    "print(train_dataset.tokenizer.decode(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int, embedding_dim: int, padding_idx: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.GRUCell(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lm_head = nn.Linear(hidden_dim, out_features=vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Obtain logits for next token.\n",
    "\n",
    "        Args:\n",
    "            x: tensor with token ids, shape (B,T,C)\n",
    "        \n",
    "        Returns:\n",
    "            logits: tensor with shape (B,T,vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            # (B,C)\n",
    "            cell_output, _ = self.rnn(x[:, t, :])\n",
    "            outputs.append(cell_output)\n",
    "        \n",
    "        # (B,T,C)\n",
    "        outputs = self.dropout(torch.stack(outputs, dim=1))\n",
    "\n",
    "        # (B,T,vocab_size)\n",
    "        return self.lm_head(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_sequences(batch: list[torch.IntTensor]) -> tuple[torch.IntTensor, torch.IntTensor]:\n",
    "    \"\"\"\n",
    "    Collate function for language modeling.\n",
    "\n",
    "    Args:\n",
    "        batch: List of tensors, where each tensor is a sequence of token IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inputs: Batch of input sequences.\n",
    "            - targets: Batch of target sequences.\n",
    "    \"\"\"\n",
    "    # (B,T)\n",
    "    batch_tensor = torch.stack(batch)\n",
    "\n",
    "    # (B,T-1)\n",
    "    inputs = batch_tensor[:, :-1]\n",
    "\n",
    "    # (B,T-1)\n",
    "    targets = batch_tensor[:, 1:]\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=2, collate_fn=collate_sequences)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=2, collate_fn=collate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Просто функции обучения модели\n",
    "# Единственное отличие от стандартной парадигмы - \n",
    "# при итерации по батчу не приходят пары (data, target),\n",
    "# а словарь с соответствубщими полями\n",
    "\n",
    "def evaluate(model, device, data_loader):\n",
    "    \"\"\"\n",
    "    Возвращает качество и лосс модели на выборке\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    length = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # Считываем данные из батча\n",
    "            data, target = batch['series'].float().to(device), batch['target'].long().to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Обновляем качество\n",
    "            loss += F.nll_loss(output, target).item() \n",
    "            pred = output.argmax(dim=1) \n",
    "            correct += (pred == target).sum().item()\n",
    "            length += batch['target'].shape[0]\n",
    "\n",
    "    return loss / length, correct / length\n",
    "\n",
    "def train(model, device, train_loader, test_loader, n_epoch, optimizer, scheduler, \n",
    "          max_norm=None, track_gradient=False):\n",
    "    \"\"\"\n",
    "    Обучение и оценивание качества на тесте одновременно\n",
    "    \n",
    "    max_norm - параметр gradient clipping. Если None, то не метод не используется.\n",
    "    track_gradient - Если true, то оцениваем градиент на каждом батче.\n",
    "    \"\"\"\n",
    "    train_history, test_history = {'loss':[], 'acc':[]}, {'loss':[], 'acc':[]}\n",
    "    \n",
    "    if track_gradient:\n",
    "        grad_history = np.zeros((n_epoch, len(list(model.parameters()))))\n",
    "    \n",
    "    for epoch in tqdm(range(1, n_epoch + 1)):\n",
    "        n_objects = 0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Считываем данные из батча\n",
    "            data, target = batch['series'].float().to(device), batch['target'].long().to(device)\n",
    "            \n",
    "            # Делаем шаг по батчу\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "                \n",
    "            # Если нужно - запоминаем норму градиента (отдельно по каждому параметру)\n",
    "            if track_gradient:\n",
    "                n_objects +=  data.shape[0]\n",
    "                for i, p in enumerate(model.parameters()):\n",
    "                    param_norm = p.grad.data.detach().norm(2)\n",
    "                    grad_history[epoch-1][i] += param_norm.item() ** 2\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Добавляем информацию в наш логгер\n",
    "        loss, acc = evaluate(model, device, train_loader)\n",
    "        train_history['loss'].append(loss), train_history['acc'].append(acc)\n",
    "        loss, acc = evaluate(model, device, test_loader)\n",
    "        test_history['loss'].append(loss), test_history['acc'].append(acc)\n",
    "        \n",
    "        if track_gradient:\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                grad_history[epoch-1][i] = (grad_history[epoch-1][i] / n_objects) ** 0.5\n",
    "       \n",
    "    \n",
    "    if track_gradient:\n",
    "        return train_history, test_history, grad_history\n",
    "    return train_history, test_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rnn-seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
