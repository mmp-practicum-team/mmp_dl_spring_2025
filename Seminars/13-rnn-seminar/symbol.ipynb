{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать посимвольную RNN на текстах Шекспира"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir shake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-18 12:21:23--  https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/refs/heads/master/datasets/shakespeare.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6347705 (6.1M) [text/plain]\n",
      "Saving to: ‘shake/shakespeare.txt’\n",
      "\n",
      "shakespeare.txt     100%[===================>]   6.05M  8.38MB/s    in 0.7s    \n",
      "\n",
      "2025-03-18 12:21:24 (8.38 MB/s) - ‘shake/shakespeare.txt’ saved [6347705/6347705]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -P shake https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/refs/heads/master/datasets/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,\\'?!]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_preprocessed(path: str) -> Path:\n",
    "    \"\"\"\n",
    "    Preprocess text and save it to disk.\n",
    "\n",
    "    Returns:\n",
    "        path to saved text\n",
    "    \"\"\"\n",
    "    path_ = Path(path)\n",
    "    with path_.open() as file:\n",
    "        text = file.read()\n",
    "    processed = preprocess(text)\n",
    "    res = path_.parent / \"processed.txt\"\n",
    "    with res.open(\"w\") as file:\n",
    "        file.write(processed)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = save_preprocessed(\"shake/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "def train_test_split(path: str, test_size: float = 0.1) -> tuple[Path, Path]:\n",
    "    \"\"\"\n",
    "    Split dataset to train and test.\n",
    "    \n",
    "    Returns:\n",
    "        train_path and test_path\n",
    "    \"\"\"\n",
    "    path_ = Path(path)\n",
    "    with path_.open() as file:\n",
    "        text = file.read()\n",
    "    test_num_chars = floor(len(text) * test_size)\n",
    "    test_text = text[-test_num_chars:]\n",
    "    train_text = text[:-test_num_chars]\n",
    "    train_path = path_.parent / \"train.txt\"\n",
    "    with train_path.open(\"w\") as file:\n",
    "        file.write(train_text)\n",
    "    test_path = path_.parent / \"test.txt\"\n",
    "    with test_path.open(\"w\") as file:\n",
    "        file.write(test_text)\n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, test_path = train_test_split(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.id2token = list(string.ascii_letters + string.digits + \".,\\'?! \\n\")\n",
    "        self.token2id = {char: token_id for token_id, char in enumerate(self.id2token)}\n",
    "    \n",
    "    def encode(self, txt: str) -> list[int]:\n",
    "        return [self.token2id[tok] for tok in txt]\n",
    "    \n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        return [self.id2token[tok_id] for tok_id in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ShakespeareTexts(Dataset):\n",
    "    def __init__(self, path: str, seq_length: int) -> None:\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = self._load_sequences(path)\n",
    "\n",
    "        self.tokenizer = CharTokenizer()\n",
    "    \n",
    "    def _load_sequences(self, path: str) -> list[str]:\n",
    "        res = []\n",
    "        with Path(path).open() as file:\n",
    "            while True:\n",
    "                sequence = file.read(self.seq_length)\n",
    "                if not sequence:\n",
    "                    break\n",
    "                res.append(sequence)\n",
    "        if len(res[-1]) != self.seq_length:\n",
    "            res.pop()\n",
    "        return res\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tokenized sequence\n",
    "        \"\"\"\n",
    "        seq = self.sequences[index]\n",
    "        token_ids = self.tokenizer.encode(seq)\n",
    "        return torch.LongTensor(token_ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177198, 19688)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ShakespeareTexts(train_path, seq_length=32)\n",
    "test_dataset = ShakespeareTexts(test_path, seq_length=32)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([53, 58, 52, 55, 68, 68, 26, 37, 37, 44, 67, 48, 30, 37, 37, 67, 45, 33,\n",
      "        26, 45, 67, 30, 39, 29, 44, 67, 48, 30, 37, 37, 68, 68])\n",
      "['1', '6', '0', '3', '\\n', '\\n', 'A', 'L', 'L', 'S', ' ', 'W', 'E', 'L', 'L', ' ', 'T', 'H', 'A', 'T', ' ', 'E', 'N', 'D', 'S', ' ', 'W', 'E', 'L', 'L', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]),\n",
    "print(train_dataset.tokenizer.decode(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(train_dataset.tokenizer.id2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_dim: int, embedding_dim: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.GRUCell(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lm_head = nn.Linear(hidden_dim, out_features=vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Obtain logits for next token.\n",
    "\n",
    "        Args:\n",
    "            x: tensor with token ids, shape (B,T)\n",
    "        \n",
    "        Returns:\n",
    "            logits: tensor with shape (B,T,vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = x.shape\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            # (B,C)\n",
    "            embedded_input = self.embedding(x[:, t])\n",
    "            cell_output = self.rnn(embedded_input)\n",
    "            outputs.append(cell_output)\n",
    "        \n",
    "        # (B,T,C)\n",
    "        outputs = self.dropout(torch.stack(outputs, dim=1))\n",
    "\n",
    "        # (B,T,vocab_size)\n",
    "        return self.lm_head(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_sequences(batch: list[torch.LongTensor]) -> tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Collate function for language modeling.\n",
    "\n",
    "    Args:\n",
    "        batch: List of tensors, where each tensor is a sequence of token IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inputs: Batch of input sequences.\n",
    "            - targets: Batch of target sequences.\n",
    "    \"\"\"\n",
    "    # (B,T)\n",
    "    batch_tensor = torch.stack(batch)\n",
    "\n",
    "    # (B,T-1)\n",
    "    inputs = batch_tensor[:, :-1]\n",
    "\n",
    "    # (B,T-1)\n",
    "    targets = batch_tensor[:, 1:]\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=2, collate_fn=collate_sequences)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=2, collate_fn=collate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model: nn.Module, device: torch.DeviceObjType, data_loader: DataLoader) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    length = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # (B,T) and (B,T)\n",
    "            inputs, targets = batch\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # (B,T,vocab_size)\n",
    "            outputs: torch.Tensor = model(inputs.to(device))\n",
    "            \n",
    "            # (B,vocab_size,T)\n",
    "            outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "            loss += F.cross_entropy(outputs, targets, reduction=\"sum\").item()\n",
    "\n",
    "            # (B,T)\n",
    "            pred = outputs.argmax(dim=1) \n",
    "\n",
    "            correct += (pred == targets).sum().item()\n",
    "            length += inputs.shape[0] * inputs.shape[1]\n",
    "\n",
    "    return loss / length, correct / length\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        device: torch.DeviceObjType,\n",
    "        train_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        n_epoch: int, optimizer: torch.optim.Optimizer,\n",
    "        max_norm: float | None = None,\n",
    "        scheduler: torch.optim.lr_scheduler.StepLR | None = None\n",
    "    ):\n",
    "    train_history, test_history = {'loss':[], 'acc':[]}, {'loss':[], 'acc':[]}\n",
    "    \n",
    "    steps_in_epoch = len(train_loader)\n",
    "\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        model.train()\n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), total=steps_in_epoch):\n",
    "            # (B,T) and (B,T)\n",
    "            inputs, targets = batch\n",
    "            \n",
    "            # (B,T,vocab_size)\n",
    "            outputs: torch.Tensor = model(inputs.to(device))\n",
    "            \n",
    "            # (B,vocab_size,T)\n",
    "            outputs = outputs.permute(0, 2, 1)\n",
    "\n",
    "            loss = F.cross_entropy(outputs, targets.to(device), reduction=\"mean\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "        loss, acc = evaluate(model, device, train_loader)\n",
    "        train_history['loss'].append(loss), train_history['acc'].append(acc)\n",
    "        print(\"Train Loss:\", loss, \"Train Accuracy:\", acc)\n",
    "\n",
    "        loss, acc = evaluate(model, device, test_loader)\n",
    "        test_history['loss'].append(loss), test_history['acc'].append(acc)\n",
    "        print(\"Val Loss:\", loss, \"Val Accuracy:\", acc)\n",
    "\n",
    "        \n",
    "\n",
    "    return train_history, test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(69, 64)\n",
       "  (rnn): GRUCell(64, 64)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lm_head): Linear(in_features=64, out_features=69, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size=vocab_size, hidden_dim=64, embedding_dim=64, dropout_rate=0.2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2285711471434215, 0.025032441572400414)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(lr=5e-4, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [00:58<00:00, 94.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.476620366579765 Train Accuracy: 0.3031628866219625\n",
      "Val Loss: 2.6422508851669715 Val Accuracy: 0.22948807854137449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [00:59<00:00, 93.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.470106620108295 Train Accuracy: 0.30306384614936466\n",
      "Val Loss: 2.6279302891232255 Val Accuracy: 0.2292799937083011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [01:00<00:00, 91.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.467459218205784 Train Accuracy: 0.3031681663530385\n",
      "Val Loss: 2.627080635522518 Val Accuracy: 0.23031058709415264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [00:59<00:00, 92.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.465727644427769 Train Accuracy: 0.30325792178133026\n",
      "Val Loss: 2.6325273085902428 Val Accuracy: 0.2296945249111953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [01:00<00:00, 92.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4647673067977425 Train Accuracy: 0.30316270456227024\n",
      "Val Loss: 2.622917025187174 Val Accuracy: 0.23010577918758437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5537/5537 [01:01<00:00, 89.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4643696782067344 Train Accuracy: 0.30284428216048054\n",
      "Val Loss: 2.625208511249535 Val Accuracy: 0.22905716270595483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 5194/5537 [00:57<00:03, 90.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, device, train_loader, test_loader, n_epoch, optimizer, max_norm)\u001b[39m\n\u001b[32m     56\u001b[39m loss = F.cross_entropy(outputs, targets.to(device), reduction=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     62\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.rnn-seminar/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.rnn-seminar/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/.rnn-seminar/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(model, device, train_loader, test_loader, n_epoch=10, optimizer=optimizer, max_norm=2, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rnn-seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
