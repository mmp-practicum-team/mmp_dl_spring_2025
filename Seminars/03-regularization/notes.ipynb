{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\renewcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\var}{\\text{Var}}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "$$\n",
    "\n",
    "# `Материалы кафедры ММП факультета ВМК МГУ. Введение в глубокое обучение.`\n",
    "\n",
    "# `Лекция 02. Регуляризация в DL`\n",
    "\n",
    "### `Материалы составил Алексеев Илья (@voorhs)`\n",
    "\n",
    "#### `Москва, Весенний семестр 2025`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Затухание градиентов`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы вывели формулу для градиентов весов линейного (полносвязного) слоя:\n",
    "$$\n",
    "z_i=W_ia_{i-1}\\Rightarrow\n",
    "\\begin{cases}\n",
    "\\nabla_{W_i}\\L=(\\nabla_{z_i}\\L)a_{i-1}^T,\\\\\n",
    "\\nabla_{a_{i-1}}\\L=W_i^T(\\nabla_{z_i}\\L).\n",
    "\\end{cases}\n",
    "$$\n",
    "А также градиенты функции активации в общем виде:\n",
    "$$\n",
    "a_i=f(z_i)\\Rightarrow\\nabla_{z_i}\\L=(\\nabla_{a_i}\\L) {\\partial a_i\\over\\partial z_i}.\n",
    "$$\n",
    "Нетрудно вывести формулу градиентов весов на $i$-ом слое ($i=\\overline{1,L}$):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{W_i}\\L&=(\\nabla_{z_i}\\L)a_{i-1}^T\\\\\n",
    "&=\\left((\\nabla_{a_i}\\L) {\\partial a_i\\over\\partial z_i}\\right)a_{i-1}^T\\\\\n",
    "&=\\left((W_{i+1}^T(\\nabla_{z_{i+1}}\\L)) {\\partial a_i\\over\\partial z_i}\\right)a_{i-1}^T\\\\\n",
    "&=\\left(\\left(W_{i+1}^T\\left((\\nabla_{a_{i+1}}\\L){\\partial a_{i+1}\\over\\partial z_{i+1}}\\right)\\right) {\\partial a_i\\over\\partial z_i}\\right)a_{i-1}^T\\\\\n",
    "&=\\ldots\\\\\n",
    "&=\\left(\\prod_{j=i+1}^LW_{j}^T\\right)(\\nabla_{a_L}\\L)\\left(\\prod_{j=i+1}^L{\\partial a_{j}\\over\\partial z_j}\\right)a_{i-1}^T.\n",
    "\\end{align}\n",
    "$$\n",
    "Эта формула является очень ценной для понимания того, в чем различие обучения первых слоев нейросети ($i=1,2,3,\\ldots$) и последних ($i=L,L-1,L-2,\\ldots$). Давайте проанализируем норму полученного градиента:\n",
    "$$\n",
    "\\|\\nabla_{W_i}\\L\\|\\leqslant \\left(\\prod_{j={i+1}}^L\\|W_j\\|\\right)\\left(\\prod_{j={i+1}}^L\\left\\|{\\partial a_j\\over\\partial z_j}\\right\\|\\right)\\|a_{i-1}\\|\n",
    "$$\n",
    "Рассмотрим случай, когда в качестве функции $a_i=f(z_i)$ выбрана сигмоида. Известно следующее:\n",
    "$$\n",
    "\\begin{cases}\n",
    "{\\partial a_j\\over\\partial z_j}=a_i(1-a_i)\\\\\n",
    "0<a_i<1\n",
    "\\end{cases}\\Rightarrow0<\\left\\|{\\partial a_j\\over\\partial z_j}\\right\\|<1\\Rightarrow\\prod_{j={i+1}}^L\\left\\|{\\partial a_j\\over\\partial z_j}\\right\\|\\xrightarrow{L\\to\\infty}0.\n",
    "$$\n",
    "То есть градиенты весов первых слоев будут значительно меньше градиентов последних слоев. Это означает две вещи:\n",
    "\n",
    "- выход нейросети практически не зависит от преобразований, совершенных на первых слоях, т.е. “информация” не доходит до конца;\n",
    "- во время оптимизации веса первых слоев будут меняться намного медленнее, чем веса последних слоев.\n",
    "\n",
    "Данное явление называют затуханием градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Функции активации`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из эффективных способов борьбы с затуханием градиентов является правильный выбор функции активации. Рассмотрим набор популярных вариантов с точки зрения их сопротивления затуханию градиентов и не только.\n",
    "\n",
    "| Функция                                                      | Затухание | Тождество | Гладкость | Сложность |\n",
    "| ------------------------------------------------------------ | --------- | --------- | --------- | --------- |\n",
    "| $\\sigma(z)=\\dfrac{1}{1+e^{-z}}$                              | FAIL      | FAIL      | OK        | FAIL      |\n",
    "| $\\tanh(z)=2\\sigma(2z)-1$                                     | FAIL      | OK        | OK        | FAIL      |\n",
    "| $\\text{ReLU}(z)=\\max\\{0,z\\}$                                 | OK        | FAIL      | OK/FAIL   | OK        |\n",
    "| $\\text{LeakyReLU}(z)=\\begin{cases}z, & z\\geqslant0,\\\\\\alpha z,& \\text{otherwise},\\end{cases}$ | OK        | OK        | OK/FAIL   | OK        |\n",
    "| $\\text{ELU}(z)=\\begin{cases}z, & z\\geqslant0,\\\\e^z-1,& \\text{otherwise}.\\end{cases}$ | OK        | OK        | OK        | FAIL      |\n",
    "| $\\text{GELU}(z)=z\\cdot\\Phi(z)$                               | ОК        | ОК        | ОК        | FAIL      |\n",
    "| $\\text{Swish}(z)=z\\cdot\\sigma(z)$                            | ОК        | ОК        | ОК        | FAIL      |\n",
    "| $\\text{Mish}(z)=z\\cdot\\tanh(\\text{softplus}(z))$             | ОК        | ОК        | ОК        | FAIL      |\n",
    "| $\\text{ReLU6}(z)=\\min(\\max(0,z),6)$                          | ОК        | FAIL      | ОК/FAIL   | OK        |\n",
    "\n",
    "### Тождественное преобразование\n",
    "\n",
    "Нейросеть с сигмоидной функцией активации нельзя научить тождественному преобразованию, т.к. вход сигмоиды — это $\\R$, а выход — это отрезок $[0,1]$. Поддерживать тождественное преобразование — это разумное требование к нейросети, т.к. иногда лучшее, что может сделать слой — это просто не испортить имеющиеся признаки.\n",
    "\n",
    "### Вычислительная сложность\n",
    "\n",
    "Сложные функции по типу синусов, экспонент и логарифмов на компьютере считаются с помощью рядов Тейлора. При большом числе слоев, число операций, требуемых для вычисления функций активации может сильно возрасти. Поэтому например выбор $\\text{ReLU}$ вместо $\\sigma$ может заметно ускорить проход по нейросети.\n",
    "\n",
    "### Гладкость\n",
    "\n",
    "Функцию называют гладкой, если её производная является непрерывной функцией. Для таких функций методы оптимизации сходятся на порядок быстрее, поэтому на первый взгляд кажется, что $\\text{ReLU},\\text{LeakyReLU}$ следует сразу вычеркнуть из списка. Но поскольку свойство гладкости нарушается лишь в одной точке, на практике методы гладкой оптимизации работают и для них. Существует даже термин: дифференцируемые в смысле нейросетей функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Нормализация`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на проблему затухающих градиентов с другой стороны. Обратим внимание на то, что если вход сигмоиды большой по модулю (очень отрицательный или очень положительный), то выход соответствует тем участкам графика, где функция очень близка к константной. Именно в этих областях числовой прямой производная близка к нулю. Поэтому для сигмоидной функции очень желательно, чтобы входы были “около нуля”.\n",
    "\n",
    "![](pics/sigmoid.svg)\n",
    "\n",
    "На самом деле, это желательно для любой функции активации и для любой нейросети. Например, если входы ReLU всегда будут отрицательными, то “сигнал” полностью оборвется. Если же они все будут положительными, то пропадает нелинейность, которую должна вносить функция активации.\n",
    "\n",
    "Привести входы слоя к промежутку “около нуля” позволяет нормализация. Вспомним, как работала нормализация признаков в классическом машинном обучении. Пусть имеется батч данных, для которых посчитаны входные активации $a\\in\\R^{B\\times d}$ какого-то промежуточного полносвязного слоя. Мы хотим нормализовать эти активации перед подачей в этот полносвязный слой.\n",
    "\n",
    "1. Считаем выборочные среднее и дисперсию для каждого признака по батчу:\n",
    "\n",
    "$$\n",
    "\\overline\\mu_{j}={1\\over B}\\sum_{i=1}^Ba_{ij},\\quad\\overline\\sigma_j^2={1\\over B}\\sum_{i=1}^B(a_{ij}-\\overline\\mu_j)^2,\\quad j=\\overline{1,d}.\n",
    "$$\n",
    "\n",
    "2. Нормализуем данные:\n",
    "\n",
    "$$\n",
    "\\tilde a_{ij}={a_{ij}-\\overline\\mu_j\\over\\overline\\sigma_j+\\varepsilon},\\quad i=\\overline{1,B},\\ j=\\overline{1,d},\\ \\varepsilon>0.\n",
    "$$\n",
    "\n",
    "Константа $\\varepsilon$ используется на всякий случай во избегание деления на ноль.\n",
    "\n",
    "Описанная конструкция применяется в классическом машинном обучении, но не пригодна для глубокого обучения по следующей причине. Размер батча $B$ как правило не так велик, чтобы получить точные оценки $\\overline\\mu,\\overline\\sigma$. Это приводит к тому, что от батча к батчу они будут сильно меняться и слой не обучится должным образом, т.к. в данных теряется много информации.\n",
    "\n",
    "Решить проблему “скачущих” статистик можно довольно простым способом — использовать скользящее среднее. Алгоритм нормализации будет следующий. Для $j=\\overline{1,d}$:\n",
    "\n",
    "0. В начале обучения заводим $\\hat\\mu_j=0$, $\\hat\\sigma_j=1$\n",
    "1. На пришедшем батче данных считаем $\\overline\\mu_j$, $\\overline\\sigma_j$\n",
    "2. Обновляем статистики (как правило $\\alpha=0.9$):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\mu_j&:=\\alpha\\hat\\mu+(1-\\alpha)\\hat\\mu_j\\\\\n",
    "\\hat\\sigma_j^2&:=\\alpha\\hat\\sigma_j^2+(1-\\alpha)\\hat\\sigma_j^2,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "3. Нормализуем данные.\n",
    "\n",
    "Описанный алгоритм уже хорош! Но стоит помнить, что даже со сглаживанием размер батча все равно должен быть достаточно большим.\n",
    "\n",
    "Данный алгоритм не дотягивает до того, что обычно называют Batch Normalization. Не хватает добавить один шаг в конце:\n",
    "$$\n",
    "\\widehat a_{ij}=\\gamma_j\\tilde a_{ij} + \\delta_j,\n",
    "$$\n",
    "где $\\gamma_j,\\delta_j\\in\\R$ — обучаемые параметры. Данный шаг мотивирован двумя соображениями:\n",
    "\n",
    "1. Обучаемое преобразование может внести корректировку в “искажение”, которое внесено нормализацией.\n",
    "2. Существуют такие $\\gamma,\\delta$, что $\\widehat a_{ij}=a_{ij}$, т.е. возможно выучить тождественное преобразование.\n",
    "\n",
    "Батч нормализация является эффективным способом борьбы с затуханием градиентов в результате насыщения функций активации.\n",
    "\n",
    "Стоит упомянуть, что помимо batch norm применяется layer norm. Разница состоит в том, что среднее и дисперсия считаются не вдоль размерности $B$, а вдоль размерности $d$. Такой метод применяется по ряду причин. Например, если вычислительные ресурсы не позволяют обрабатывать достаточно большой батч, чтобы посчитанные статистики были достаточно точными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Инициализация весов`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к выражению для нормы градиентов весов:\n",
    "$$\n",
    "\\|\\nabla_{W_i}\\L\\|\\leqslant \\underbrace{\\left(\\prod_{j={i+1}}^L\\|W_j\\|\\right)}_{\\text{init.}}\\underbrace{\\left(\\prod_{j={i+1}}^L\\left\\|{\\partial a_j\\over\\partial z_j}\\right\\|\\right)}_\\text{act.fun., norm.}\\underbrace{\\|a_{i-1}\\|}_\\text{norm.}\n",
    "$$\n",
    "Проанализируем методы, которые мы успели ввести выше.\n",
    "\n",
    "- Выбор функции активации влияет на второй множитель справа. \n",
    "- Нормализация направлена на второй и третий множитель справа.\n",
    "\n",
    "Как повлиять на первый множитель? Если нормы матриц будут слишком маленькими, то градиент затухнет, а если большими, — то взорвется. Чтобы побороть это, необходимо выбрать правильную инициализацию весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Инициализация Xavier Glorot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $z=Wx$, где $W\\in\\R^{n_\\text{in}\\times n_\\text{out}}$. Знаем, что $\\L'_x=W^T\\L'_z$. Допустим, на вход подается нормированная выборка:\n",
    "$$\n",
    "x_i\\stackrel{iid}\\sim N(0,\\var(x))\n",
    "$$\n",
    "Найдем самый разумный способ инициализировать веса $w_{ij}$. Будем искать среди нормальных распределений с неизвестным параметром $\\var(w)$:\n",
    "$$\n",
    "w_{ij}\\stackrel{iid}\\sim N(0,\\var(w)).\n",
    "$$\n",
    "При проходе вперед мы хотели бы сохранить нормированность выборки. Заметим, что при проходе вперед\n",
    "\n",
    "- сохраняется центрированность выборки:\n",
    "\n",
    "$$\n",
    "\\E z_i=\\E\\bigg(\\sum_j w_{ij} x_j\\bigg)=\\sum_j(\\E w_{ij})(\\E x_j)=\\{\\E w_{ij}=\\E x_j=0\\}=0.\n",
    "$$\n",
    "\n",
    "- изменяется дисперсия выборки:\n",
    "\n",
    "$$\n",
    "\\var(z)=\\var\\bigg(\\sum_j w_{ij}x_j\\bigg)=\\sum_j\\var(w_{ij})\\var(x_j)=n_\\text{in}\\var(w)\\var(x).\n",
    "$$\n",
    "\n",
    "При проходе назад мы бы хотели сохранить нормированность градиентов, т.к. это сохранит нормированность выборки. Заметим, что при проходе назад\n",
    "\n",
    "- сохраняется центрированность градиентов:\n",
    "\n",
    "$$\n",
    "\\E\\left({\\L'_{x_i}}\\right)=\\E\\bigg(\\sum_jw_{ji}\\L'_{z_j}\\bigg)=\\sum_j(\\E w_{ji})\\E \\bigg(\\L'_{z_j}\\bigg)=\\{\\E w_{ij}=0\\}=0\n",
    "$$\n",
    "\n",
    "- изменяется дисперсия:\n",
    "\n",
    "$$\n",
    "\\var\\left(\\L'_{x_i}\\right)=n_\\text{out}\\var(w)\\var\\left(L'_{z_j}\\right).\n",
    "$$\n",
    "\n",
    "Значит, центрированность всегда сохраняется, осталось потребовать, чтобы не менялась дисперсия. Т.е.\n",
    "$$\n",
    "\\var(z)=\\var(x),\\quad \\var\\left(\\L'_x\\right)=\\var\\left(\\L'_z\\right).\n",
    "$$\n",
    "Получили систему относительно $\\var(w)$.\n",
    "$$\n",
    "\\begin{cases}\n",
    "n_\\text{in}\\var(w)=1, \\\\\n",
    "n_\\text{out}\\var(w)=1.\n",
    "\\end{cases}\n",
    "$$\n",
    "Эта система несовместна, но в качестве компромисса можем взять\n",
    "$$\n",
    "\\var(w)={2\\over n_\\text{in}+n_\\text{out}}.\n",
    "$$\n",
    "Это первый вариант инициализации. Второй состоит в том, что использовать не нормальное, а равномерно распределение, которое ищется из тех же соображений:\n",
    "$$\n",
    "w_{ij}\\stackrel{iid}\\sim \\text{Unif}\\left[-{1\\over\\sqrt n},{1\\over\\sqrt n}\\right],\\quad n={n_\\text{in}+n_\\text{out}\\over6}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Инициализация для tanh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию активации удовлетворяющую следующим условиям:\n",
    "1. Нечётность, то есть $g(x) = -g(-x)$\n",
    "2. $g^{\\prime}(0) = 1$\n",
    "\n",
    "Так, этим условиям удовлетворяет Tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша цель избежать затухания градиентов в процессе обучения за счёт правильной начальной инициализации.\n",
    "\n",
    "Рассмотрим модель нейросети — MLP с функцией активации $g$. Положим $a^{L}, b^{L} \\in \\mathbb{R}^{n^{L} \\times 1}$ — активации и вектор сдвига слоя $L$, $W^{L} \\in \\mathbb{R}^{n^{L}\\times n^{L-1}}$ -- веса слоя $L$, $x \\in \\mathbb{R}^{n^{0}}$ — вектор входных данных.\n",
    "$$\n",
    "a^{0} = x\n",
    "$$\n",
    "$$\n",
    "z^{L} = W^{L}a^{L-1} + b^{L}\n",
    "$$\n",
    "$$\n",
    "a^{L} = g(z^{L})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем следующие предположения:\n",
    "1. Все вектора сдвига равны нулю: $b^{L} = 0 \\; \\forall L$\n",
    "2. В начале обучения модель находится в *линейном режиме*, то есть $z^{L}_{j} \\approx 0 \\Rightarrow f(z^{L}) \\approx z^{L}$\n",
    "3. $W^{L}_{jk}, a^{L}_{j}, z^{L}_{j}, x_{j}, \\delta^{L}_{j} = \\frac{\\partial\\mathfrak{L}}{\\partial z^{L}_{j}}$ — распределены независимо, при этом распределение каждой группы переменных для одного слоя зависит только от номера этого слоя и одинаково для всех переменных.\n",
    "4. $\\mathbb{E}x = \\mathbb{E}x_{j} = 0, \\mathbb{D}x = \\mathbb{D}x_{j} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этих предположениях рассмотрим прямой и обратный проход по сети. Рассмотрим, как меняется распределения активаций от слоя к слою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Прямой проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прямом проходе мы хотим добиться того, чтобы матожидание и дисперсия активаций оставались неизменными от слоя к слою. Тем самым мы гарантируем что активации не будут \"взрываться\" или \"испаряться\" к последним слоям сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что из второго предположения следует, что матожидания всех активаций равно 0. Рассмотрим дисперсию активаций:\n",
    "$$\n",
    "\\mathbb{D}z^{L} = \\mathbb{D}z^{L}_{j} = \\mathbb{D} \\sum\\limits_{i=1}^{n^{L-1}} W^{L}_{jk}a^{L-1}_{k} = \\sum\\limits_{i=1}^{n^{L-1}} \\mathbb{D}W^{L}_{jk}\\mathbb{D}a^{L-1}_{k} = n^{L-1}\\mathbb{D}W^{L}\\mathbb{D}a^{L-1}\n",
    "$$\n",
    "Учитывая второе условие:\n",
    "$$\n",
    "\\mathbb{D}z^{L} = n^{L-1}\\mathbb{D}W^{L}\\mathbb{D}a^{L-1} = n^{L-1}\\mathbb{D}W^{L}\\mathbb{D}z^{L-1} = n^{L-1}\\mathbb{D}W^{L}n^{L-2}\\mathbb{D}W^{L-1}\\mathbb{D}z^{L-2} = ... = \\mathbb{D}x\\prod\\limits_{l=0}^{L-1}n^{l}\\mathbb{D}W^{l+1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{D}z^{L} = \\mathbb{D}z^{L-1} \\iff n^{L - 1}\\mathbb{D}W^{L} = 1 \\iff \\mathbb{D}W^{L} = \\frac{1}{n^{L-1}} \\;\\;\\;\\; (6)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Обратный проход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прямом проходе мы хотим добиться того, чтобы матожидание и дисперсия градиентов оставались неизменными от слоя к слою. Тем самым мы гарантируем что градиенты не будут \"взрываться\" или \"испаряться\" к первым слоям сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta^{L}_{j} = \\sum\\limits_{k=1}^{n^{L+1}}\\delta^{L+1}_{k}\\frac{\\partial z^{L+1}_{k}}{\\partial z^{L}_{j}} = \\sum\\limits_{k=1}^{n^{L+1}}\\delta^{L+1}_{k}\\frac{\\partial }{\\partial z^{L}_{j}}\\sum\\limits_{i=1}^{n^{L}}W^{L+1}_{ki}g(z^{L_{i}}) = \\sum\\limits_{k=1}^{n^{L+1}}\\delta^{L+1}_{k}W^{L+1}_{kj}\\frac{\\partial g(z^{L_{j}})}{\\partial z^{L}_{j}} = \\sum\\limits_{k=1}^{n^{L+1}}\\delta^{L+1}_{k}W^{L+1}_{kj}g^{\\prime}(z^{L_{j}})\n",
    "$$\n",
    "\n",
    "Учитывая, что $g^{\\prime}(x) \\approx 1$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\delta^{L} = \\mathbb{E}\\delta^{L}_{j} = n^{L+1}\\mathbb{E}W^{L+1}\\mathbb{E}\\delta^{L+1} = 0 \\Leftarrow \\mathbb{E}W^{L} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{D}\\delta^{L} = \\mathbb{D}\\delta^{L}_{j} = n^{L+1}\\mathbb{D}W^{L+1}\\mathbb{D}\\delta^{L+1} = n^{L+1}\\mathbb{D}W^{L+1}n^{L+2}\\mathbb{D}W^{L+2}\\mathbb{D}\\delta^{L+2} = ... = \\mathbb{D}\\delta^{d}\\sum\\limits_{m=L+1}^{d-1}n^{m}\\mathbb{D}W^{m}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{D}\\delta^{L} = \\mathbb{D}\\delta^{L-1} \\iff n^{L}\\mathbb{D}W^{L} = 1 \\iff \\mathbb{D}W^{L} = \\frac{1}{n^{L}} \\;\\;\\;\\; (7)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, $(6), (7)$ могут выполняться одновременно тогда и только тогда, когда $n^{L} = n^{L+1} \\; \\forall L$, то есть когда ширина MLP остаётся постоянной. В ином случае, предлагается брать среднее гармоническое полученных оценок:\n",
    "$$\n",
    "\\begin{cases} \n",
    "    \\mathbb{E}W^{L} = 0\\\\\n",
    "    \\mathbb{D}W^{L} = \\frac{2}{n^{L-1} + n^{L}} = \\frac{2}{n^{\\text{in}} + n^{\\text{out}}}\n",
    "\\end{cases} \\;\\;\\;\\;\\; (8)\n",
    "$$\n",
    "\n",
    "В зависимости от того, что важнее — сохранение дисперсии активаций или сохранение дисперсий градиентов можно использовать или (6), или (7), но обычно, дисперсия градиентов важнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество распределений, для которых $(8)$ выполняется. Стандартным вариантом является использование равномерного распределения на подходящем интервале:\n",
    "$$\n",
    "W^{L}_{ij} \\sim \\mathcal{U}[-\\sqrt{\\frac{6}{n^{\\text{in}} + n^{\\text{out}}}}, \\sqrt{\\frac{6}{n^{\\text{in}} + n^{\\text{out}}}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные результаты могут быть получены для других функций активации — ReLU, Sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ортогональная инициализация`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущий метод инициализации основан на идее сохранения дисперсии и среднего. Следующий вывод\n",
    "\n",
    "Инициализируем веса $W\\in\\R^{n\\times p}$ так, чтобы столбцы $W$ были ортогональны, т.е. $W^TW=I$. Тогда\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\|z\\|_2=\\|Wx\\|_2=\\|x\\|_2,\\\\\n",
    "& \\|\\nabla_x\\L\\|_2=\\|W^T\\nabla_z\\L\\|_2=\n",
    "\\|\\nabla_z\\L\\|_2.\n",
    "\\end{align}\n",
    "$$\n",
    "Т.е. нормы сохраняются. \n",
    "\n",
    "Как сохранить ортогональность матрицы $W$ во время оптимизации? При оптимизации градиентными методами, матрица $W$ выйдет из множества ортогональных при первом же шаге. Чтобы предотвратить это, используют прием под названием репараметризация.\n",
    "\n",
    "Справедливо следующее свойство. Для случайной $V\\in\\R^{d\\times d}$, матрица $W=\\exp(V-V^T)$ является ортогональной, где $\\text{exp}$ — матричная экспонента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Skip-connection / Residual Connection / DenseNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним способом регуляризации нейросетей является “прокидывание” связей, или использование остаточных связей. Идея простая: давайте переиспользовать признаки с предыдущих слоев.\n",
    "\n",
    "Переиспользовать признаки можно разными способами:\n",
    "\n",
    "1. Поэлементно прибавлять предыдущие активации к текущим.\n",
    "2. Конкатенировать предыдущие активации к текущим.\n",
    "\n",
    "Такое прокидывание связей помогает решить несколько проблем:\n",
    "\n",
    "- затухание градиентов, поскольку через прокинутую связь градиент “протекает” без изменений, т.к. это тождественное преобразование\n",
    "- потеря признаков, потому что каждый линейный слой получает совершенно новые признаки; это небережливо по отношению к признакам с глубоких слоев, которые могли оказаться полезными\n",
    "- устранение симметрии полносвязных слоев; подробнее об этой гипотезе смотрите здесь: https://habr.com/ru/articles/688350/\n",
    "- сглаживание ландшафта функции потерь; подбобнее смотрите здесь: https://arxiv.org/abs/1712.09913\n",
    "\n",
    "![](pics/skip-connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Drop Out`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: на этапе обучения в каждом слое будем “отключать” случайно выбранные нейроны, т.е.\n",
    "$$\n",
    "\\begin{align}\n",
    "& z=Wx+b \\\\\n",
    "& a:=g(z) \\\\\n",
    "& y_i\\sim\\text{Bern}(1-p) \\\\\n",
    "& a^\\text{almost DO} := a\\odot y\n",
    "\\end{align}\n",
    "$$\n",
    "Но в таком случае $\\E a^\\text{DO}=(1-p)\\E a$. Поэтому dropout-значение нужно поделить на $1-p$:\n",
    "$$\n",
    "\\begin{align}\n",
    "& z=Wx+b \\\\\n",
    "& a:=g(z) \\\\\n",
    "& y_i\\sim\\text{Bern}(1-p) \\\\\n",
    "& a^\\text{DO} := a\\odot y/(1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "Авторы метода утверждают, что такая процедура исключает эффект коадаптации: когда верхние слои просто копируют предыдущие и ничему не учатся. “Выключение” случайных входных нейронов приводит к тому, что утилизируются все нейроны какие только есть, т.е. через них текут градиенты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Стохастическая оптимизация`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении нейросетей используются градиентные методы, причем стохастические. Оказывается, что это объясняется не только оптимизацией вычислительных ресурсов (избегание вычисления точного градиента по всей выборке), но и эффектом регуляризации, который привносит “шумный” градиент.\n",
    "\n",
    "Рассмотрим на примере оптимизации MSE в задаче регрессии по одному признаку методом SGD с размером батча равным одному объекту. MSE выглядит так:\n",
    "$$\n",
    "\\L={1\\over \\ell}\\sum_{i=1}^\\ell(wx_i-y)^2,\n",
    "$$\n",
    "где $y_i\\in\\R$ — истинная метка, $w\\in\\R$ — обучаемый вес, $x_i\\in\\R$ — единственный признак. Каждое слагаемое является параболой относительно $x_i$. Мы можем представить процесс оптимизации методом SGD следующим образом: на каждом шаге случайно выбирается одна парабола, делается шаг $\\eta\\cdot x_i(wx_i-y)$, который направлен в сторону вершины этой параболы (т.к. это оптимум для нее). Если мы верим, что задача регрессии имеет красивое решение, то вершины всех парабол находятся примерно около друг друга и есть какое-то “среднее” оптимальное место. Когда значение $w$ далеко от этого места, SGD безошибочно идет в его сторону. Но когда SGD приближается к оптимуму, есть вероятность, что случайно выбранная парабола будет в направлении, противоположном оптимуму. Поэтому SGD никогда не сойдется к оптимуму, вместо этого он будет оссциллировать в регионе неопределенности.\n",
    "\n",
    "Это свойство SGD является недостатком с точки зрения методов оптимизации. Но мы решаем не просто задачу оптимизации, а задачу машинного обучения, в которой как правило запрещено идеально подгоняться под обучающую выборку. С этой точки зрения SGD вносит регуляризацию.\n",
    "\n",
    "Это свойство особенно полезно в контексте глубокого обучения, потому что для многослойных сетей ландшафт функции потерь представляет собой минное поле с разбросанными минами — плохими локальными оптимумами. Стохастическая оптимизация позволяет обходить узкие локальные оптимумы, в то время как точный градиент просто ищет любой ближайший.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
