# Трансформер

## Вспоминаем RNN

На прошлой лекции вы узнали про принципы работы RNN:
- обучаемые преобразования для обработки последовательностей в рекуррентном режиме 
- shared parameters на каждом рекуррентном шаге
- контекстуализация векторных представлений слов
- механизм внимания в автокодировщике как дополнительный механизм памяти помимо рекуррентного вектора контекста

Мы вводили рекуррентную ячейку:

![rnn-cell](./figures/rnn-rnn-cell.drawio.svg)

Она имеет два входа $h,x$ и два выхода $h',y$:
$$
h'=g_h(W_1x+W_2h),\quad
y=g_y(W_3h'),
$$

Одним из применений была задача seq2seq моделирования с помощью автокодировщика:

![rnn-seq2seq](./figures/rnn-translation.drawio.svg)

Давайте запишем псевдокод алгоритма работы кодировщика и декодировщика


### Алгоритм "RNN Encoder"

---

ВХОД: $e(w_1), \ldots, e(w_T)$

ВЫХОД: $y_1,\ldots, y_T$

---

1. $h_0 := 0$.
2. **Для** $ t $ от 1 до $ T $:
   - $ h_t, y_t := \text{RNNCell}(h_{t-1}, e(w_t)) $.

---

### Алгоритм "RNN Decoder" (в режиме обучения)

---

ВХОД: $e(\text{BOS}), e(\tilde w_1), \ldots, e(\tilde w_n)$, $h_T$

ВЫХОД: $z_0, z_1, \ldots, z_n$

---

1. $s_0 := h_T$.
2. **Для** $ t $ от 0 до $ n $:
   - $ s_t, z_t := \text{RNNCell}(s_{t-1}, e(\tilde w_{t})) $.

---

### Механизм внимания


> Рисуем атеншен


$$
s_t=g_s(W_1z_t+W_2s_{t-1}+W_3c_t),\quad c_t=\sum_{i=1}^Th_i\alpha_{it},\\
\alpha_{it}={\exp(\text{sim}(s_{t-1},h_i))\over\sum_{j=1}^T\exp(\text{sim}(s_{t-1},h_j))},\quad\text{sim}(x,y)=\text{FC}(x,y).
$$


### Алгоритм "RNN Decoder с механизмом внимания"


**ВХОД:**  
- $ e(\text{BOS}), e(\tilde w_1), \ldots, e(\tilde w_n) $  
- $ h_1, h_2, \ldots, h_T $ (encoder hidden states)  
- $ h_T $ (initial decoder hidden state)  

**ВЫХОД:**  
- $ z_0, z_1, \ldots, z_n $  

---

1. $ s_0 := h_T $.  
2. **Для** $ t $ от 0 до $ n $:  
    - **Для** $ i $ от 1 до $ T $:  
    $$
    \alpha_{it} := \frac{\exp(\text{sim}(s_{t-1}, h_i))}{\sum_{j=1}^T \exp(\text{sim}(s_{t-1}, h_j))}
    $$  
    - $c_t := \sum_{i=1}^T h_i \alpha_{it}$
   - $s_t, z_t := \text{RNNCell}(s_{t-1}, e(\tilde w_{t}), c_t).$

---

На этой лекции мы разовьем идею механизма внимания таким образом, что пройдем путь от RNN до трансформера --- архитектуры, в которой все рекуррентные связи заменены механизмом внимания.

## Механизм внимания формально

Проанализируем эти формулы с точки зрения векторных операций. Алгоритм подсчёта атеншена для одного шага следующий:

---
ВХОД:
- $q\in\mathbb{R}^d$  [запрос, например $s_t$]
- $K\in\mathbb{R}^{T\times d}$ [ключи, например $h_1,\ldots,h_T$]
- $V\in\mathbb{R}^{T\times d}$ [значения, например $h_1,\ldots,h_T$]
---
ВЫХОД:
- $c\in\mathbb{R}^d$ [собранный контекст, например $c_t$]

---

1. $m := Kq$
2. $\alpha:=\text{Softmax}(m)$
3. $c:=V^T\alpha$

---

Операцию, задаваемую этим алгоритмом, обозначим $\text{Attention}(q,K,V)$. Алгоритм декодировщика можно записать так:

**ВХОД:**  
- $ e(\text{BOS}), e(\tilde w_1), \ldots, e(\tilde w_n) $  
- $ H=[h_1, h_2, \ldots, h_T ]$ (encoder hidden states)  
- $ h_T $ (initial decoder hidden state)  

**ВЫХОД:**  
- $ z_0, z_1, \ldots, z_n $  

---

1. $ s_0 := h_T $.  
2. **Для** $ t $ от 0 до $ n $:  
    - $c_t := \text{Attention}(s_{t-1}, H, H)$
    - $s_t, z_t := \text{RNNCell}(s_{t-1}, e(\tilde w_{t}), c_t).$

---

## Механизм самовнимания

Обратимся к псевдокоду и схеме RNN с атеншеном. Мы видим, что в этом алгоритме есть два способа прокидывания связей между словами: рекуррентные формулы и атеншен пулинг. Идея механизма самовнимания в том, чтобы убрать рекуррентные связи и заменить их атеншен пулингом. Сделаем это аккуратно, распишем отдельно для кодировщика и для декодировщика.

### Самовнимание в кодировщике

Попытаемся сначала придумать наивный алгоритм, как можно заменить рекуррентные связи на механизм внимания.

---

ВХОД: $E=[e(w_1), \ldots, e(w_T)]\in\mathbb{R}^{T\times d}$

ВЫХОД: $y_1,\ldots, y_T$

---

1. **Для** $ t $ от 1 до $ T $:
    - $ c_t := \text{Attention}(e(w_t), E, E)$
    - $ y_t := \text{FC}(c_t) $.

---

В данном случае мы заменили $\text{RNNCell}$ на связку $\text{Attention}+\text{FC}$.

Мы успешно заменили рекуррентные связи в кодировщике. Принципиальное достоинство этого алгоритма в следующем:
- каждое слово в последовательности (кодировщика или декодировщика) имеет непосредственную связь со всеми другими словами в этой последовательности (а не как было в RNN: однонаправленная связь или двунаправленная связь с непосредственными соседями). Т.е. последовательность "смотрит" на саму себя, отсюда название алгоритма

Далее, ключевая идея в том, чтобы обнаружить пространство для оптимизации: заметьте, что все итерации цикла можно посчитать независимо. В функцию $\text{Attention}$ можно подать в качестве $q$ не один вектор $e(w_t)$, а сразу всю последовательность $E$. В алгоритме атеншена изменится лишь то, что вместо умножения $Kq\in\mathbb{R}^{T}$ будет умножение $QK^T\in\mathbb{R}^{T\times T}$.

---
ВХОД:
- $Q\in\mathbb{R}^{T\times d}$
- $K\in\mathbb{R}^{T\times d}$
- $V\in\mathbb{R}^{T\times d}$
---
ВЫХОД:
- $C\in\mathbb{R}^{T\times d}$

---

1. $m := QK^T$
2. $A:=\text{Softmax}(m)$  [построчно]
3. $C:=A V$

---

Если записывать коротко: $\text{Attention}(Q,K,V)=\text{Softmax}(QK^T)V$. Полный алгоритм кодировщика тогда следующий:

---

ВХОД: $E=[e(w_1), \ldots, e(w_T)]\in\mathbb{R}^{T\times d}$

ВЫХОД: $Y=[y_1,\ldots, y_T]\in\mathbb{R}^{T\times d}$

---

1. $ C := \text{Attention}(E, E, E)$
2. $ Y := \text{FC}(C) $.

---

> Картинка с блоком кодировщика

### Самовнимание в декодировщике

Как сделать все то же самое в декодировщике? Наивно, это можно сделать так:

---

**ВХОД:**  
- $ \tilde E=[e(\text{BOS}), e(\tilde w_1), \ldots, e(\tilde w_n) ]$  
- $ H=[h_1, h_2, \ldots, h_T ]$ (encoder hidden states)  

**ВЫХОД:**  
- $ z_0, z_1, \ldots, z_n $  

---

1. **Для** $ t $ от 0 до $ n $:  
    - $s_t := \text{Attention}(e(\tilde w_t), \tilde E, \tilde E)$ [самовнимание]
    - $c_t := \text{Attention}(s_t, H, H)$ [кросс-внимание]
    - $z_t := \text{FC}(c_t)$.

---

Первое, что мы можем улучшить, это добавить параллелизм, поскольку итерации цикла независимы:

---

**ВХОД:**  
- $ \tilde E=[e(\text{BOS}), e(\tilde w_1), \ldots, e(\tilde w_n) ]$  
- $ H=[h_1, h_2, \ldots, h_T ]$ (encoder hidden states)

**ВЫХОД:**  
- $ Z=[z_0, z_1, \ldots, z_n ]$  

---


1. $S := \text{Attention}(\tilde E, \tilde E, \tilde E)$
2. $C := \text{Attention}(S, H, H)$
3. $Z := \text{FC}(C)$.

---

Отлично! Но этот алгоритм ничему полезному не научится. Надо вспомнить, что декодировщик учится на задачу Causal Language Modeling. В строке 1. самовнимание работает на всю последовательность и во все обе стороны, прямо как в кодировщике. Это означает, что в активации $z_t$, по которой мы планирует делать предсказание токена $\tilde w_{t+1}$ уже будет содержаться информация о $e(\tilde w_{t+1})$. Происходит утечка меток.

> Псевдокод маскированного самовнимания для декодировщика

Вы помните, что помимо самовнимания в декодировщике ещё используется внимание, обращённое к выходам кодировщика. В контексте трансформеров это внимание называют кросс-вниманием, т.е. одна последовательность "смотрит" на другую.

## Обучаемое внимание

В оригинальном атеншене для рекуррентных сетей были обучаемые параметры. Они были зашиты в функции sim(x,y)=FC(x,y) --- полносвязной сети которая принимает на вход конкатенацию х и у. Это делает операцию атеншен пулинга обучаемой, что добавляет гибкости и репрезентативной мощности архитектуре в целом. В введённом нами алгоритме используется sim(x,y)=⟨x,y⟩. Как можно было бы ввести обучаемые параметры в нее?

> Псевдокод атеншена с параметрами W_q, W_k, W_v

## Блок кодировщика 

До сих пор мы говорили лишь про механизм внимания. Однако мы ещё не построили полноценную замену RNN, потому что помимо коммуникации с помощью вектора контекста, в RNN была ещё и независимая обработка каждого токена.

Введем конструкцию, которую называют слоем трансформера кодировщика.

> Картинка блока кодировщика
> Псевдокод блока кодировщика 

Ключевые приемы:
- архитектура "коммуникация + обработка" за счёт использования position wise feed forward с shared parameters
- прокидывания связей
- нормализация

## Блок декодировщика

У декодировщика ситуация аналогичная

> Картинка блока декодировщика
> Псевдокод блока декодировщика

## Трансформер автокодировщик 

> Картинка

Основная идея: использовать много блоков в кодировщике и декодировщике.

## Вычислительная сложность трансформера 

Если взглянуть на псевдокод RNN и посчитать число операций и потребление памяти, то получится О(Т), где Т --- длина последовательности. У трансформера в механизме внимания возникает перемножение матриц Q и K, поэтому сложность трансформера и потребление памяти O(T^2).

Как так? Мы получили что новая архитектура поедает колоссально много ресурсов, особенно в случае экстра длинных последовательностей. Однако может показаться парадоксом, но за одно и то же время трансформер способен пройти большее число шагов оптимизации, чем RNN. Дело в том, что в алгоритме RNN присутсвует цикл по t, т.е. алгоритм принципиально последовательный, а такое медленно считается на GPU. В трансформере же все сведено к матричному перемножению --- эта операция эффективно параллелизуется на GPU и позволяет утилизировать больше вычислительной мощности. Т.е. трансформеру действительно нужно сделать больше операций с вещественными числами, но эти операции намного быстрее.

> Тут добавить про SIMD, HBM, read/write и cache missing, memory access patterns, throughput vs latency

Однако существует много архитектурных приемов ускорения трансформеров. Почти все они направлены на эффективное вычисление и хранение матриц внимания.

## Многоголовочность

Когда говорят про трансформеры, почти всегда подразумевают использование трюка с использованием многоголовочности атеншена.

> Формулы

Благодаря этому механизму возникает два эффекта:
- ансамблирование
- узкое горлышко

## Полносвязный блок

Блок position wise feed forward обладает одинаковой структурой во всех трансформерах: это двухслойная сеть, которая из активации размерности d получает активацию размерности 4d и затем обратно d. 

> Картинка
> Формула

Смысл такой архитектуры двуликий:
- Мягкий словарь. Эти блоки составляют подавляющую часть всех параметров трансформера. Логично предположить что в них хранится большая часть знаний трансформера как языковой модели о внешнем мире.
> Анализ формул и интерпретация как мязкого словаря
- Утилизация GPU. Такие сети называют широкими. Мы можем противопоставить ей глубокую сеть: что если взять полносвязную сеть с тем же числом параметров, но сделать много узких слоев вместо двух широких? Эта архитектура плоха по той же причине, почему плох RNN: лучше использовать одну большую но параллелизуемую операцию, чем много маленьких но последовательных.

В оригинальном трансформере и в целом в трансформерах долгое время использовались функции активации наподобие ReLU и GELU. Сейчас используются SwiGLU.

> Формулы SwiGLU

## Позиционная кодировка

Заметим одну примечательную особенность: атеншен обрабатывает множество, а не последовательность. В рекуррентных сетях порядок слов учитывался в самом алгоритме за счёт использования цикла по t. Сейчас же мы имеем только перемножение матриц. В текущем виде информация о порядке слов нигде не учитывается.

Многие годы трансформеры использовали прием под названием позиционная кодировка. Идея в том, чтобы к входным эмбедингам слов добавлять специальный эмбединг, соответствующий номеру слова в предложении. Оригинальный трансформер использовал фиксированные эмбединги, получаемые с помощью синусов и косинусов. Затем сообщество открыло, что эти эмбединги можно просто инициализировать случайными и обучать. Сейчас трансформеры в большинстве своем вообще отошли от этой идеи и используются методы которые напрямую влияют механизм подсчёта атеншена.

> Объяснить RoPE

