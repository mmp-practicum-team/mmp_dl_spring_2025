{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b9580a",
   "metadata": {},
   "source": [
    "# `Материалы кафедры ММП факультета ВМК МГУ. Введение в глубокое обучение.`\n",
    "\n",
    "# `Лекция 08. Трансформеры`\n",
    "\n",
    "##### `Материалы составил Алексеев Илья (@voorhs)`\n",
    "\n",
    "#### `Москва, Весенний семестр 2025`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb1b19",
   "metadata": {},
   "source": [
    "## `Вспоминаем RNN`\n",
    "\n",
    "На прошлой лекции вы узнали про принципы работы RNN:\n",
    "- обучаемые преобразования для обработки последовательностей в рекуррентном режиме \n",
    "- shared parameters на каждом рекуррентном шаге\n",
    "- контекстуализация векторных представлений слов\n",
    "- механизм внимания в автокодировщике как дополнительный механизм памяти помимо рекуррентного вектора контекста\n",
    "\n",
    "Мы вводили рекуррентную ячейку:\n",
    "\n",
    "![rnn-cell](./figures/rnn-rnn-cell.drawio.svg)\n",
    "\n",
    "Она имеет два входа $h,x$ и два выхода $h',y$:\n",
    "$$\n",
    "h'=g_h(W_1x+W_2h),\\quad\n",
    "y=g_y(W_3h'),\n",
    "$$\n",
    "\n",
    "Одним из применений была задача seq2seq моделирования с помощью автокодировщика:\n",
    "\n",
    "![rnn-seq2seq](./figures/rnn-translation.drawio.svg)\n",
    "\n",
    "Давайте запишем псевдокод алгоритма работы кодировщика и декодировщика\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37402e39",
   "metadata": {},
   "source": [
    "### Алгоритм \"RNN Encoder\"\n",
    "\n",
    "---\n",
    "\n",
    "ВХОД:\n",
    "\n",
    "- $E=[e(w_1), \\ldots, e(w_T)]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "ВЫХОД:\n",
    "\n",
    "- $Y=[y_1,\\ldots, y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "- $H=[h_1,\\ldots, h_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. $h_0 := 0$.\n",
    "2. **Для** $ t $ от 1 до $ T $:\n",
    "   - $ h_t, y_t := \\text{RNNCell}(h_{t-1}, e(w_t)) $.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88eddfa",
   "metadata": {},
   "source": [
    "### Алгоритм \"RNN Decoder\" (в режиме обучения)\n",
    "\n",
    "---\n",
    "\n",
    "ВХОД:\n",
    "\n",
    "- $\\tilde E=[e(\\text{BOS}), e(\\tilde w_1), \\ldots, e(\\tilde w_{n-1})]\\in\\mathbb{R}^{n\\times d}$\n",
    "- $h_T\\in\\mathbb{R}^d$\n",
    "\n",
    "ВЫХОД:\n",
    "\n",
    "- $Z=[z_0, z_1, \\ldots, z_{n-1}]\\in\\mathbb{R}^{n\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. $s_{-1} := h_T$.\n",
    "2. $e(\\tilde w_0):=e(\\text{BOS})$\n",
    "3. **Для** $ t $ от 0 до $ n -1$:\n",
    "   - $ s_t, z_t := \\text{RNNCell}(s_{t-1}, e(\\tilde w_{t})) $.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfd17c",
   "metadata": {},
   "source": [
    "### `Механизм внимания`\n",
    "\n",
    "![](./figures/attention.drawio.svg)\n",
    "\n",
    "$$\n",
    "s_t=g_s(W_1z_t+W_2s_{t-1}+W_3c_t),\\quad c_t=\\sum_{i=1}^Th_i\\alpha_{it},\\\\\n",
    "\\alpha_{it}={\\exp(\\text{sim}(s_{t-1},h_i))\\over\\sum_{j=1}^T\\exp(\\text{sim}(s_{t-1},h_j))},\\quad\\text{sim}(x,y)=\\langle x,y\\rangle.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17454da8",
   "metadata": {},
   "source": [
    "### Алгоритм \"RNN Decoder с механизмом внимания\"\n",
    "\n",
    "\n",
    "**ВХОД:**  \n",
    "- $\\tilde E=[e(\\text{BOS}), e(\\tilde w_1), \\ldots, e(\\tilde w_{n-1})\\in\\mathbb{R}^{n\\times d}$  \n",
    "- $H=[h_1, h_2, \\ldots, h_T]\\in\\mathbb{R}^{T\\times d}$ \n",
    "- $Y=[y_1,\\ldots,y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "**ВЫХОД:**  \n",
    "\n",
    "- $Z=[z_0, z_1, \\ldots, z_{n-1}]\\in\\mathbb{R}^{n\\times d}$  \n",
    "\n",
    "---\n",
    "\n",
    "1. $s_{-1} := h_T$.\n",
    "2. $e(\\tilde w_0):=e(\\text{BOS})$\n",
    "\n",
    "3. **Для** $ t $ от 0 до $ n -1$:  \n",
    "\n",
    "   - **Для** $ i $ от 1 до $ T $:  \n",
    "   $$\n",
    "   \\alpha_{it} := \\frac{\\exp(\\text{sim}(s_{t-1}, h_i))}{\\sum_{j=1}^T \\exp(\\text{sim}(s_{t-1}, h_j))}\n",
    "   $$\n",
    "   - $c_t := \\sum_{i=1}^T h_i \\alpha_{it}$\n",
    "   - $s_t, z_t := \\text{RNNCell}(s_{t-1}, e(\\tilde w_{t}), c_t).$\n",
    "\n",
    "---\n",
    "\n",
    "На этой лекции мы разовьем идею механизма внимания таким образом, что пройдем путь от RNN до трансформера — архитектуры, в которой все рекуррентные связи заменены механизмом внимания.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6a35",
   "metadata": {},
   "source": [
    "## `Механизм внимания в векторной форме`\n",
    "\n",
    "Проанализируем эти формулы с точки зрения векторных операций. Алгоритм подсчёта атеншена для одного шага следующий:\n",
    "\n",
    "---\n",
    "ВХОД:\n",
    "- $q\\in\\mathbb{R}^d$  [запрос, например $s_t$]\n",
    "- $K\\in\\mathbb{R}^{T\\times d}$ [ключи, например $h_1,\\ldots,h_T$]\n",
    "- $V\\in\\mathbb{R}^{T\\times d}$ [значения, например $y_1,\\ldots,y_T$]\n",
    "---\n",
    "ВЫХОД:\n",
    "- $c\\in\\mathbb{R}^d$ [собранный контекст, например $c_t$]\n",
    "\n",
    "---\n",
    "\n",
    "1. $m := Kq$\n",
    "2. $\\alpha:=\\text{Softmax}(m)$\n",
    "3. $c:=V^T\\alpha$\n",
    "\n",
    "---\n",
    "\n",
    "Операцию, задаваемую этим алгоритмом, обозначим $\\text{Attention}(q,K,V)$. Алгоритм декодировщика можно записать так:\n",
    "\n",
    "---\n",
    "\n",
    "1. $s_{-1} := h_T$.\n",
    "2. $e(\\tilde w_0):=e(\\text{BOS})$\n",
    "3. **Для** $ t $ от 0 до $ n -1$:  \n",
    "    - $c_t := \\text{Attention}(s_{t-1}, H, H)$\n",
    "    - $s_t, z_t := \\text{RNNCell}(s_{t-1}, e(\\tilde w_{t}), c_t).$\n",
    "\n",
    "---\n",
    "\n",
    "Обратимся к псевдокоду и схеме RNN с атеншеном. Мы видим, что в этом алгоритме есть два способа прокидывания связей между словами: рекуррентные формулы и атеншен пулинг. Идея механизма самовнимания в том, чтобы убрать рекуррентные связи и заменить их атеншен пулингом. Сделаем это аккуратно, распишем отдельно для кодировщика и для декодировщика.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69ebb2",
   "metadata": {},
   "source": [
    "### `Самовнимание в кодировщике`\n",
    "\n",
    "Попытаемся сначала придумать наивный алгоритм, как можно заменить рекуррентные связи на механизм внимания.\n",
    "\n",
    "---\n",
    "\n",
    "ВХОД:\n",
    "\n",
    "- $E=[e(w_1), \\ldots, e(w_T)]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "ВЫХОД:\n",
    "\n",
    "- $Y=[y_1,\\ldots, y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "- $H=[h_1,\\ldots, h_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. **Для** $ t $ от 1 до $ T $:\n",
    "    - $ c_t := \\text{Attention}(e(w_t), E, E)$\n",
    "    - $ y_t := \\text{FC}(c_t) $.\n",
    "\n",
    "---\n",
    "\n",
    "В данном случае мы заменили $\\text{RNNCell}$ на связку $\\text{Attention}+\\text{FC}$.\n",
    "\n",
    "Мы успешно заменили рекуррентные связи в кодировщике. Принципиальное достоинство этого алгоритма в следующем:\n",
    "- каждое слово в последовательности (кодировщика или декодировщика) имеет непосредственную связь со всеми другими словами в этой последовательности (а не как было в RNN: связь с непосредственными соседями).\n",
    "- Т.е. последовательность \"смотрит\" на саму себя, отсюда название алгоритма\n",
    "\n",
    "Далее, ключевая идея в том, чтобы обнаружить пространство для оптимизации: заметьте, что все итерации цикла можно посчитать независимо. В функцию $\\text{Attention}$ можно подать в качестве $q$ не один вектор $e(w_t)$, а сразу всю последовательность $E$. В алгоритме атеншена изменится лишь то, что вместо умножения $Kq\\in\\mathbb{R}^{T}$ будет умножение $QK^T\\in\\mathbb{R}^{T\\times T}$.\n",
    "\n",
    "---\n",
    "ВХОД:\n",
    "- $Q\\in\\mathbb{R}^{T\\times d}$\n",
    "- $K\\in\\mathbb{R}^{T\\times d}$\n",
    "- $V\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "ВЫХОД:\n",
    "- $C\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. $m := QK^T/\\sqrt{d}$\n",
    "2. $A:=\\text{Softmax}(m)$  [построчно]\n",
    "3. $C:=A V$\n",
    "\n",
    "---\n",
    "\n",
    "Если записывать коротко:\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{Softmax}(QK^T)V.\n",
    "$$\n",
    "Полный алгоритм кодировщика тогда следующий:\n",
    "\n",
    "---\n",
    "\n",
    "ВХОД:\n",
    "\n",
    "- $E=[e(w_1), \\ldots, e(w_T)]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "ВЫХОД:\n",
    "\n",
    "- $Y=[y_1,\\ldots, y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. $ C := \\text{Attention}(E, E, E)$\n",
    "2. $ Y := \\text{FC}(C) $.\n",
    "\n",
    "---\n",
    "\n",
    "![](./figures/encoder-block-naive.drawio.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382ed2f",
   "metadata": {},
   "source": [
    "### `Самовнимание в декодировщике`\n",
    "\n",
    "Как сделать все то же самое в декодировщике? Наивно, это можно сделать так:\n",
    "\n",
    "---\n",
    "\n",
    "**ВХОД:**  \n",
    "- $ \\tilde E=[e(\\text{BOS}), e(\\tilde w_1), \\ldots, e(\\tilde w_{n-1}) ]\\in\\mathbb{R}^{n\\times d}$  \n",
    "- $H=[h_1, h_2, \\ldots, h_T]\\in\\mathbb{R}^{T\\times d}$ \n",
    "- $Y=[y_1,\\ldots,y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "**ВЫХОД:**  \n",
    "\n",
    "- $Z=[z_0, z_1, \\ldots, z_{n-1}]\\in\\mathbb{R}^{n\\times d}$  \n",
    "\n",
    "---\n",
    "\n",
    "1. $e(\\tilde w_0):=e(\\text{BOS})$\n",
    "1. **Для** $ t $ от 0 до $ n -1$: \n",
    "    - $s_t := \\text{Attention}(e(\\tilde w_t), \\tilde E, \\tilde E)$ [самовнимание]\n",
    "    - $c_t := \\text{Attention}(s_t, H, Y)$ [кросс-внимание]\n",
    "    - $z_t := \\text{FC}(c_t)$.\n",
    "\n",
    "---\n",
    "\n",
    "Первое, что мы можем улучшить, это добавить параллелизм, поскольку итерации цикла независимы:\n",
    "\n",
    "---\n",
    "\n",
    "**ВХОД:**  \n",
    "- $ \\tilde E=[e(\\text{BOS}), e(\\tilde w_1), \\ldots, e(\\tilde w_{n-1}) ]\\in\\mathbb{R}^{n\\times d}$  \n",
    "- $H=[h_1, h_2, \\ldots, h_T]\\in\\mathbb{R}^{T\\times d}$ \n",
    "- $Y=[y_1,\\ldots,y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "**ВЫХОД:**  \n",
    "- $ Z=[z_0, z_1, \\ldots, z_{n-1} ]\\in\\mathbb{R}^{n\\times d}$  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. $S := \\text{Attention}(\\tilde E, \\tilde E, \\tilde E)$\n",
    "2. $C := \\text{Attention}(S, H, Y)$\n",
    "3. $Z := \\text{FC}(C)$.\n",
    "\n",
    "---\n",
    "\n",
    "Отлично! Вы помните, что помимо самовнимания в декодировщике ещё используется внимание, обращённое к выходам кодировщика. В контексте трансформеров это внимание называют кросс-вниманием, т.е. одна последовательность \"смотрит\" на другую.\n",
    "\n",
    "![](figures/decoder-block-naive.drawio.svg)\n",
    "\n",
    "Правда этот алгоритм ничему полезному не обучится. Надо вспомнить, что декодировщик учится на задачу Causal Language Modeling. В строке 1. самовнимание работает на всю последовательность и в обе стороны, прямо как в кодировщике. Это означает, что в активации $z_t$, по которой мы планирует делать предсказание токена $\\tilde w_{t+1}$ уже будет содержаться информация о $e(\\tilde w_{t+1})$. Происходит утечка меток.\n",
    "\n",
    "Назовем следующий алгоритм операцией $\\text{MaskedAttention}(Q,K,V)$\n",
    "\n",
    "---\n",
    "\n",
    "ВХОД:\n",
    "- $Q\\in\\mathbb{R}^{T\\times d}$\n",
    "- $K\\in\\mathbb{R}^{T\\times d}$\n",
    "- $V\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "ВЫХОД:\n",
    "- $C\\in\\mathbb{R}^{T\\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "1. $m := QK^T/\\sqrt{d}$\n",
    "2. $m := \\text{triu}(d, -\\infty)$ [верхняя треугольная матрица $d\\times d$ со значениями $-\\infty$]\n",
    "3. $A:=\\text{Softmax}(m)$  [построчно]\n",
    "4. $C:=A V$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff519cf",
   "metadata": {},
   "source": [
    "### `Обучаемое внимание`\n",
    "\n",
    "В оригинальном атеншене для рекуррентных сетей были обучаемые параметры. Они были зашиты в функции $\\text{sim}(x,y)=\\text{FC}(x,y)$ — полносвязной сети которая принимает на вход конкатенацию $x$ и $y$. Это делает операцию атеншен пулинга обучаемой, что добавляет гибкости и репрезентативной мощности архитектуре в целом. В введённом нами алгоритме используется $\\text{sim}(x,y)=\\langle x,y\\rangle$. Как можно было бы ввести обучаемые параметры в нее?\n",
    "\n",
    "Введем линейные преобразованием перед операцией атеншена:\n",
    "$$\n",
    "\\text{SelfAttention}(X)=\\text{Attention}(XW_q,XW_k,XW_v),\\\\\n",
    "\\text{CrossAttention}(X,Y)=\\text{Attention}(XW_q,YW_k,YW_v),\n",
    "$$\n",
    "где $W_q,W_k,W_v\\in\\mathbb{R}^{d\\times d}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21256ba8",
   "metadata": {},
   "source": [
    "## `Нормализация и skip-connection`\n",
    "\n",
    "![](./figures/encoder-block.drawio.svg)\n",
    "\n",
    "---\n",
    "ВХОД:\n",
    "- $X=[x_1,\\ldots,x_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "ВЫХОД:\n",
    "- $Y=[y_1,\\ldots,y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "1. $C:=\\text{LayerNorm}(X+\\text{SelfAttention}(X))$\n",
    "4. $Y:=\\text{LayerNorm}(C+\\text{FC}(C))$\n",
    "---\n",
    "\n",
    "У декодировщика ситуация аналогичная\n",
    "\n",
    "![](./figures/decoder-block.drawio.svg)\n",
    "\n",
    "---\n",
    "ВХОД:\n",
    "- $\\tilde X=[\\tilde x_1,\\ldots,\\tilde x_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "- $Y=[y_1,\\ldots,y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "ВЫХОД:\n",
    "- $\\tilde Y=[\\tilde y_1,\\ldots,\\tilde y_T]\\in\\mathbb{R}^{T\\times d}$\n",
    "---\n",
    "1. $S:=\\text{LayerNorm}(\\tilde X + \\text{MaskedSelfAttention}(\\tilde X))$\n",
    "1. $S:=\\text{LayerNorm}(S + \\text{CrossAttention}(S, Y))$\n",
    "5. $\\tilde Y:=\\text{LayerNorm}(S+\\text{FC}(S))$\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cb2cb",
   "metadata": {},
   "source": [
    "## `Вычислительная сложность трансформера `\n",
    "\n",
    "Если взглянуть на псевдокод RNN и посчитать число операций и потребление памяти, то получится $O(T)$, где $T$ — длина последовательности. У трансформера в механизме внимания возникает перемножение матриц $QK^T$, поэтому сложность трансформера и потребление памяти $O(T^2)$.\n",
    "\n",
    "Как так? Мы получили что новая архитектура поедает колоссально много ресурсов, особенно в случае экстра длинных последовательностей. Однако может показаться парадоксом, но за одно и то же время трансформер способен пройти большее число шагов оптимизации, чем RNN. Дело в том, что в алгоритме RNN присутсвует цикл по $t$, т.е. алгоритм принципиально последовательный, а такое медленно считается на GPU. В трансформере же все сведено к матричному перемножению --- эта операция эффективно параллелизуется на GPU и позволяет утилизировать больше вычислительной мощности. Т.е. трансформеру действительно нужно сделать больше операций с вещественными числами, но эти операции намного быстрее.\n",
    "\n",
    "Это объясняется архитектурными особенностями GPU:\n",
    "\n",
    "- SIMD: single instruction multiple data\n",
    "- Иерархия памяти: read/write и cache missing\n",
    "- High-throughput, а не low-latency\n",
    "\n",
    "<img src=\"./figures/gpu-memory-hirerarchy.png\" style=\"zoom:33%;\" />\n",
    "\n",
    "Однако существует много архитектурных приемов ускорения трансформеров. Почти все они направлены на эффективное вычисление и хранение матриц внимания.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8ee9d",
   "metadata": {},
   "source": [
    "## `Многоголовочность`\n",
    "\n",
    "Когда говорят про трансформеры, почти всегда подразумевают использование трюка с использованием многоголовочности атеншена.\n",
    "\n",
    "$$\n",
    "\\text{head}_i=\\text{Attention}(Q\\underbrace{W_q}_{d\\times {d\\over n}},K\\underbrace{W_k}_{d\\times {d\\over n}},V\\underbrace{W_v}_{d\\times {d\\over n}})\\in\\mathbb{R}^{T\\times {d\\over n}}, \\quad i=\\overline{1,n},\\\\\n",
    "\\text{MultiHeadAttention}(Q,K,V)=\\text{concat}(\\text{head}_1,\\ldots,\\text{head}_n)\\underbrace{W_o}_{d\\times d}\n",
    "$$\n",
    "\n",
    "Благодаря этому механизму возникает два эффекта:\n",
    "- ансамблирование\n",
    "- узкое горлышко\n",
    "- еще больше параллелизма\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653df8b",
   "metadata": {},
   "source": [
    "## `Полносвязный блок`\n",
    "\n",
    "Блок position wise feed forward обладает одинаковой структурой почти во всех трансформерах: это двухслойная сеть, которая из активации размерности $d$ получает активацию размерности $4d$ и затем обратно $d$. \n",
    "\n",
    "![](./figures/ff.drawio.svg)\n",
    "$$\n",
    "\\text{FC}(X):=g(X\\underbrace{W_1}_{d\\times {4d}}+b_1)\\underbrace{W_2}_{4d\\times d}+b_2.\n",
    "$$\n",
    "\n",
    "Смысл такой архитектуры двоякий:\n",
    "- Мягкий словарь. Эти блоки составляют подавляющую часть всех параметров трансформера. Логично предположить что в них хранится большая часть знаний трансформера как языковой модели о внешнем мире.\n",
    "- Утилизация GPU. Такие сети называют широкими. Мы можем противопоставить ей глубокую сеть: что если взять полносвязную сеть с тем же числом параметров, но сделать много узких слоев вместо двух широких? Эта архитектура плоха по той же причине, почему плох RNN: лучше использовать одну большую но параллелизуемую операцию, чем много маленьких но последовательных.\n",
    "\n",
    "В оригинальном трансформере и в целом в трансформерах долгое время использовались функции активации наподобие ReLU и GELU. Сейчас все чаще используются SwiGLU (особенно в LLM):\n",
    "\n",
    "1. $h_1:=XW_1+b_1$ [расширение $d\\to 4d$]\n",
    "2. $h_2:=XW_2+b_2$ [расширение $d\\to 4d$]\n",
    "3. $h_\\text{SwiGLU}:=h_1\\circ\\sigma(h_1)\\circ h_2$ [рамерность $4d$]\n",
    "4. $\\text{FC}(X):=h_\\text{SwiGLU}W_3+b_3$ [сужение $4d\\to d$]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3614ec5",
   "metadata": {},
   "source": [
    "## `Позиционная кодировка`\n",
    "\n",
    "Заметим одну примечательную особенность: атеншен обрабатывает множество, а не последовательность. В рекуррентных сетях порядок слов учитывался в самом алгоритме за счёт использования цикла по t. Сейчас же мы имеем только перемножение матриц, порядок слов отражается только в порядке строк в этих матрицах, но не в самих признаках. В текущем виде информация о порядке слов нигде не учитывается.\n",
    "\n",
    "Трансформеры в базовом варианте используют прием под названием позиционная кодировка. Идея в том, чтобы ко входным эмбедингам слов добавлять специальный эмбединг, соответствующий номеру слова в предложении. Оригинальный трансформер использовал фиксированные эмбединги, получаемые с помощью синусов и косинусов. Затем сообщество открыло, что эти эмбединги можно просто инициализировать случайными и обучать. Сейчас продвинутые трансформеры вообще отошли от этой идеи и используют методы, которые напрямую влияют на механизм подсчёта атеншена.\n",
    "\n",
    "Синусоидная кодировка состоит в том, чтобы завести матрицу $P\\in\\mathbb{R}^{T\\times d}$, где $T$ — максимальная длина последовательностей, которые наша модель будет поддерживать. Матрица определяется следующими правилами:\n",
    "$$\n",
    "P_{m,2i}:=\\sin\\left({m\\over10000^{2i/d}}\\right),\\quad\n",
    "P_{m,2i+1}:=\\cos\\left({m\\over10000^{2i/d}}\\right),\n",
    "$$\n",
    "для всех $i=\\overline{1,d/2}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb27237",
   "metadata": {},
   "source": [
    "## `Полная схема трансформера`\n",
    "\n",
    "<img src=\"./notes.assets/transformer.jpg\" style=\"zoom: 33%;\" />\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
