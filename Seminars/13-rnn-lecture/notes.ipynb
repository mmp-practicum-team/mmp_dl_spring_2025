{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919ddcd0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "# `Материалы кафедры ММП факультета ВМК МГУ. Введение в глубокое обучение.`\n",
    "\n",
    "# `Леуция 07. Рекуррентные нейронные сети`\n",
    "\n",
    "### `Материалы составил Алексеев Илья`\n",
    "\n",
    "#### `Москва, Весенний семестр 2025`\n",
    "\n",
    "[Советуем так же ознамиться с материалами предыдущих лет](https://github.com/mmp-practicum-team/mmp_practicum_spring_2024/blob/main/Seminars/Seminar%2006/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5%20%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D0%A1%D0%B5%D1%82%D0%B8.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62837dcc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Вспомним w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399e17e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "На прошлой неделе вы познакомились с языковой моделью word2vec. Это модель получения эмбедингов для отдельных токенов. Её можно применять как для слов, так и для текстов. Чтобы получить эмбединг текста с помощью word2vec, достаточно усреднить эмбединги токенов, входящих в текст. Схематически это выглядит следующим образом:\n",
    "\n",
    "![](notes.assets/rnn-w2v.drawio.svg)\n",
    "\n",
    "Такую модель можно даже дообучать, чтобы адаптировать word2vec эмбединги под конкретную задачу. Однако у данной модели есть существенный недостаток. Рассмотрим пример предложения: “Робин съел лук”. Слово “лук” — известный пример омонимии. Смысл этого токена нельзя понять без контекста, в котором он встретился. В данном случае, скорее всего, это овощ. Но эмбединг слова “лук”, полученный методом word2vec, будет передавать информацию обо всех значениях этого токена, а не только ту её узкую и более точную часть, которая относится к текущему контексту."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ebc35",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34403157",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Введем модель, которую назовем рекуррентной ячейкой:\n",
    "\n",
    "![image-20250316131041975](./notes.assets/rnn-rnn-cell.drawio.svg)\n",
    "\n",
    "Рекуррентная ячейка имеет два входа $h,x$ и два выхода $h',y$:\n",
    "$$\n",
    "h'=g_h(W_1x+W_2h),\\quad\n",
    "y=g_y(W_3h'),\n",
    "$$\n",
    "где $g_h(\\cdot),g_y(\\cdot)$ — функции активации. Этот функциональный элемент действует очень похоже на полносвязную сеть, только помимо потока “вперед”, в ней реализован поток “вправо”.\n",
    "$$\n",
    "h',y=\\text{RNNCell}(h,x).\n",
    "$$\n",
    "Рекуррентная ячейка “смешивает” векторы $h$ и $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a801a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3acd1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Применим рекуррентную ячейку к целому тексту путем обработки всех токенов одной и той же рекуррентной ячейкой последовательно:\n",
    "\n",
    "![image-20250316131756463](./notes.assets/rnn-rnn.drawio.svg)\n",
    "$$\n",
    "h_t,y_t=\\text{RNNCell}(h_{t-1},x_t),\\quad t=\\overline{1,T}.\n",
    "$$\n",
    "Векторы $h_t$ будем называть скрытыми состояниями. Вектор $h_0$ инициализируется нулевым. Совершая проход по предложению, рекуррентная сеть сохраняет в $h$ информацию о всех токенах. А именно, в $h_t$ “примешана” информация о всех токенах $x_1,\\ldots,x_t$.\n",
    "\n",
    "Самое главное, что в результате такого прогона получается последовательность $y_1,\\ldots,y_T$ — эти векторы можно воспринимать как репрезентации токенов $x_1,\\ldots, x_T$, обогащенные контекстной информацией."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c2ef7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Применение и обучение RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bca70",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Необходимо разобраться, как обучать рекуррентную сеть таким образом, чтобы в контекст $h$ и в обновленные репрезентации $y$ записывалась информация, полезная для решения конкретных задач NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87476b22",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb911a0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Чтобы обучить рекуррентную сеть на задачу классификации, можно применить усреднение обновленных репрезентаций:\n",
    "\n",
    "![image-20250316133313712](./notes.assets/rnn-clf.drawio.svg)\n",
    "\n",
    "В данной схеме все операции дифференцируемы, поэтому обучая классификационную голову на кросс-энтропийный лосс, мы получим градиенты и для рекуррентного блока. Заметим, что поскольку везде применяется одна и та же рекуррентная ячейка, то обучаемыми параметрами являются только 5 матриц: $W_1,W_2,W_3,W_4$ и матрица в классификационной голове. Это сближает рекуррентные сети со сверточными в том смысле, что обе архитектуры основаны на идее применения одних и тех же (разделенных) параметров к разным частям входа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b773c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Causal Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e362c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Или задача предсказания следующего токена. Это генеративная задача. Сначала объясним, как это должно работать, и только потом, как это обучать.\n",
    "\n",
    "На вход такой задаче приходит начало предложения, которое называют затравкой (prompt). Задача продолжить это предложение. Допустим, в качестве затравки используется “Однажды я съездил в Питер и со мной случилось следующее:”, тогда задача модели сгенерировать текст, который может выдумать, что с ней случилось в Питере.\n",
    "\n",
    "Рекуррентные сети справляются с такими задачами путем авторегрессионного инференса:\n",
    "\n",
    "![image-20250316134110796](./notes.assets/rnn-clm.drawio.svg)\n",
    "\n",
    "На этой схеме классификатор везде один и тот же и он выдает вероятности по всем токенам в вокабулярии. Это вероятности каждого токена быть следующим в предложении. Предсказав токен $e_{T+1}$, его можно подать на вход рекуррентной ячейке (и соответственно классификатору), которая предскажет токен $e_{T+2}$. И так далее.\n",
    "\n",
    "Термин “авторегрессия” взят из задачи предсказания временных рядов, где тоже часто используют свои предыдущие предсказания для получения все новых и новых.\n",
    "\n",
    "Остается вопрос: как обучить рекуррентную сеть для работы в авторегрессионном режиме? Для этого достаточно собрать большой корпус текстов. Один обучающий пример формируется следующим образом. Допустим у нас есть текст $x_1,\\ldots,x_T$. Тогда\n",
    "\n",
    "1. прогоним через RNN текст $w_1,\\ldots,w_{T-1}$ и получим $y_1,\\ldots,y_{T-1}$\n",
    "2. пропустим игреки через классификатор и получим логиты\n",
    "3. используем кросс-энтропийный лосс, где истинными метками будут  $w_2,\\ldots,w_T$ \n",
    "\n",
    "В следующих занятиях будет рассказано больше практических соображений о том, как формировать обучающую выборку и батчи для обучения языковой модели для генерации текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ea6ed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Распознавание именованных сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c988e46",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Именованными сущностями (named entity) называют адрес, имя, время и прочее. Обычно они представляют некоторую информацию, которую нужно извлечь из текста. Попытаемся пойти от простого к сложному. Допустим, мы хотим извлекать человеческие имена вроде Илья, Петя, Гриша. Самый простой способ — это бинарная классификация каждого токена на предмет того, является ли он человеческим именем:\n",
    "\n",
    "![image-20250316145559488](./notes.assets/rnn-ner.drawio.svg)\n",
    "\n",
    "Чтобы обучить такую модель, достаточно собрать корпус текстов и сделать разметку для каждого токена.\n",
    "\n",
    "Рассмотрим пример сложнее: распознавание адресов вроде “ул. Строителей, дом 9”. Главная сложность в том, что  именованная сущность распростерлась на несколько токенов, каждый из которых не является именованной сущностью. В таких случаях вместо бинарной классификации используют классификацию на три класса: beginning, inside, outside (так называемая BIO-разметка). Каждый токен мы классифицируем на три класса:\n",
    "\n",
    "- B: первый токен именованной сущности\n",
    "- I: токен, относящийся к именованной сущности, но не являющийся первым\n",
    "- O: токен, не относящийся к именованной сущности\n",
    "\n",
    "Рассмотрим пример еще сложнее: распознавание и человеческих имен, и адресов. Сложность в том, что нужно уметь извлекать разные сущности с помощью одной модели. Решение простое: заведем BIO классы для каждой сущности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84316f05",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69c01f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "RNN можно применять для перевода с языка на язык (например, с английского на французский). Для этого используются так называемые автокодровщики, с идеей которых вы можете быть знакомы, если помните как решались img-to-img задачи. Сейчас мы будем решать задачу text-to-text.\n",
    "\n",
    "Допустим, мы хотим обучить модель для перевода с английского на французский. На инференсе должно работать в следующем режиме:\n",
    "\n",
    "- прогоним английский текст $x_1,\\ldots,x_T$ через RNN и получим $h_T$ — скрытое состояние RNN после токена $x_T$\n",
    "- возьмем вторую RNN сеть, которая будет генерировать перевод в авторегрессионном режиме\n",
    "- дли этого подадим ей в качестве затравки служебный токен `[BOS]` (означает beginning of sequence), и в качестве начального вектора контекста $h_T$\n",
    "- генерируем перевод до тех пор, пока языковая голова не предскажет токен `[EOS]` (означает end of sequence)\n",
    "\n",
    "Чтобы обучить такую модель достаточно применить тот же рецепт, что и для обучения causal languge model.\n",
    "\n",
    "Первую RNN сеть (которая кодирует $x_1,\\ldots,x_T$ в вектор $h_T$) называют кодировщиком. Вторую RNN сеть (которая из $h_T$ генерирует перевод в авторегрессионном режиме) называют декодировщиком.\n",
    "\n",
    "![image-20250316152710814](./notes.assets/rnn-translation.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4833c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Модификации RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a4830",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Очевидный недостаток обсуждаемой модели в том, что рекуррентное движение происходит только слева направо. Последние токены предложения “видят” первые, но не наоборот. Чтобы исправить это, достаточно добавить вторую RNN сеть, которая будет совершать свой путь в обратном направлении. Тогда в качестве обновленной репрезентации токена $x_t$ можно взять $(y_t+\\tilde y_t)$, где $y_t$ — выход из первой сети, а $\\tilde y_t$ — выход из второй.\n",
    "\n",
    "![](notes.assets/rnn-bidirectional.drawio.svg) \n",
    "\n",
    "\n",
    "\n",
    "Второй очевидной модификаций является многослойная RNN сеть. Каждый слой будет выполнять ту же функцию что и в MLP и в свёрточных сетях: генерация все более высокоуровневых и полезных для задачи признаков.\n",
    "\n",
    "![](notes.assets/rnn-multilayer.drawio.svg)\n",
    "\n",
    "Еще одна модификация, которая может сработать, это прокидывание остаточных связей между слоями.\n",
    "\n",
    "![](notes.assets/rnn-skip-connection.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d486c1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Регуляризация в RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ccec4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Обучать RNN довольно тяжело на длинных последовательностях из-за угасания и взрыва градиентов. Для борьбы с этим существует стандартный набор трюков: нормализация, дропаут, правильная токенизация. Однако со своими нюансами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed139bd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960fff4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Во-первых, для текстов используют не batch-нормализацию, а layer-нормализацию. \n",
    "\n",
    "Вход $x$ представляет собой тензор размера `(batch_size, sequence_length, hidden_size)`. Батч-норм считает статистики вдоль размерности батча для каждого признака каждого токена\n",
    "\n",
    "$$\n",
    "\\mu_{h} = \\frac{1}{BT} \\sum_{i,t=1}^{B,T} x_{ith}\\\\\n",
    "\\sigma_{h}^2 = \\frac{1}{BT} \\sum_{i,t=1}^{B,T} (x_{ith} - \\mu_{h})^2\\\\\n",
    "y_{ith} = \\gamma_h\\frac{x_{ith} - \\mu_{h}}{\\sqrt{\\sigma_{h}^2 + \\epsilon}}+\\delta_h\n",
    "$$\n",
    "В задачах с текстом обычно батч небольшой, а тексты имеют разную длину, поэтому статистики посчитанные таким образом могут быть неточными.\n",
    "\n",
    "Layer-нормализация считает статистики вдоль размерности признака, поэтому лишена этих недостатков:\n",
    "\n",
    "$$\n",
    "\\mu_{it} = \\frac{1}{H} \\sum_{h=1}^H x_{ith}\\\\\n",
    "\\sigma_{it}^2 = \\frac{1}{H} \\sum_{h=1}^H (x_{ith} - \\mu_{it})^2\\\\\n",
    "y_{ith} = \\gamma_h\\frac{x_{ith} - \\mu_{it}}{\\sqrt{\\sigma_{it}^2 + \\epsilon}}+\\delta_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef50e47",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee550e3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "С этим вы хорошо познакомитесь в домашке. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22924c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201281b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Очевидно, что чем длиннее входная последовательность, тем сложнее решать задачи с ней. На длину последовательности влияет способ токенизации текста. Рассмотрим два крайних случая\n",
    "\n",
    "**Токен = символ.**\n",
    "\n",
    "- плюс: маленький вокабулярий (=число поддерживаемых символов) => нужно относительно мало выходов классификатора \n",
    "- минус: вход относительно длинный\n",
    "\n",
    "**Токен = слово.**\n",
    "\n",
    "- плюс: вход относительно короткий\n",
    "- минус: огромный вокабулярий который все равно не охватит все слова языка\n",
    "\n",
    "Балансом между этими крайними случаями является так называемые BPE-токенизация. \n",
    "\n",
    "---\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "1. **Инициализация:**\n",
    "\n",
    "   - Пусть текст $ T $ разбит на слова $ W = \\{w_1, w_2, \\dots, w_n\\} $.\n",
    "   - Разбей каждое слово $ w_i $ на символы: $ w_i = \\{c_1, c_2, \\dots, c_m\\} $.\n",
    "   - Построй частотный словарь $ D $, где $ D[c_j] $ — частота символа $ c_j $.\n",
    "\n",
    "2. **Обучение:**\n",
    "   - Повторяй до достижения желаемого размера словаря $ |D| $:\n",
    "     - Найди наиболее частую пару $ (x, y) $ в $ D $:\n",
    "       $$\n",
    "       (x, y) = \\arg\\max_{(x, y) \\in D} \\text{count}(x, y)\n",
    "       $$\n",
    "     - Создай новый субтокен $ z = x + y $.\n",
    "     - Обнови словарь $ D $:\n",
    "       $$\n",
    "       D[z] = \\text{count}(x, y)\\\\\n",
    "       D[x] = D[x] - \\text{count}(x, y)\\\\\n",
    "       D[y] = D[y] - \\text{count}(x, y)\n",
    "       $$\n",
    "     - Удали пару $ (x, y) $ из словаря, если их частота стала нулевой.\n",
    "\n",
    "3. **Токенизация:**\n",
    "\n",
    "   - Для каждого слова $ w_i $ в тексте $ T $:\n",
    "     - Разбей $ w_i $ на символы: $ w_i = \\{c_1, c_2, \\dots, c_m\\} $.\n",
    "     - Итерируйся по символам, объединяя их в субтокены, используя словарь $ D $:\n",
    "       $$\n",
    "       \\text{tokenize}(w_i) = \\arg\\max_{z \\in D} \\text{count}(z)\n",
    "       $$\n",
    "     - Если токен не найден, используй специальный токен (например, `<unk>`).\n",
    "\n",
    "**Пример:**\n",
    "\n",
    "Пусть текст: \"low lower lowest\".\n",
    "\n",
    "1. Инициализация:\n",
    "   - Слова: [\"low\", \"lower\", \"lowest\"].\n",
    "   - Частотный словарь: {\"l\": 3, \"o\": 3, \"w\": 3, \"e\": 2, \"r\": 1, \"s\": 1, \"t\": 1}.\n",
    "\n",
    "2. Обучение:\n",
    "   - Наиболее частая пара: \"l\" и \"o\" → \"lo\".\n",
    "   - Новый словарь: {\"lo\": 3, \"w\": 3, \"e\": 2, \"r\": 1, \"s\": 1, \"t\": 1}.\n",
    "   - Следующая пара: \"lo\" и \"w\" → \"low\".\n",
    "   - Новый словарь: {\"low\": 3, \"e\": 2, \"r\": 1, \"s\": 1, \"t\": 1}.\n",
    "\n",
    "3. Токенизация:\n",
    "   - \"low\" → [\"low\"].\n",
    "   - \"lower\" → [\"low\", \"e\", \"r\"].\n",
    "   - \"lowest\" → [\"low\", \"e\", \"s\", \"t\"].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1ba3e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Механизм внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d9e83",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Обратимся к задаче перевода (text-to-text). У предложенного нами решения с помощью RNN есть очевидный недостаток: во время авторегрессионного предсказания перед декодером стоит слишком сложная задача сгенерировать целый текст имея на вход всего лишь супер сжатое представление исходного текста в виде одного вектора. Что если позволить декодеру в каждый момент генерации видеть не только вектор контекста $h$, но и эмбединги исходной последовательности? Тогда декодер смог бы совершать перевод например так, как это делают люди: смотря на конкретную часть исходного текста и подбирая слова для перевода. Такую возможность дает механизм внимания.\n",
    "\n",
    "Посмотрим внимательно на формулу RNN ячейки:\n",
    "$$\n",
    "s_t=g_s(W_1z_t+W_2s_{t-1}),\\quad y_t=g_y(W_3s_t).\n",
    "$$\n",
    "Активация $y_t$ используется как признаки для предсказания следующего токена $z_{t+1}$. При этом $y_t$ получается только из текущего токена перевода $z_t$ и из вектора контекста $s_{t-1}$. Что если мы модифицируем сумму в скобках:\n",
    "$$\n",
    "s_t=g_s(W_1z_t+W_2s_{t-1}+W_3d_t),\\quad d_t=\\sum_{i=1}^Th_i\\alpha_{it},\n",
    "$$\n",
    "где $\\alpha_{it}$ — некоторые веса, характеризующие полезность $i$-го токена исходной последовательности для предсказания $(t+1)$-го токена перевода. Если мы сможем для каждого шага $t$ подбирать нужные веса, получится реализовать умную агрегацию информации из исходной последовательности, которую можно назвать механизмом внимания (токены перевода уделяют разное внимание исходным токенам).\n",
    "\n",
    "Простейший вариант определения весов $\\alpha$ следующий:\n",
    "$$\n",
    "\\alpha_{it}={\\exp(\\text{sim}(s_{t-1},h_i))\\over\\sum_{j=1}^T\\exp(\\text{sim}(s_{t-1},h_j))},\\quad\\text{sim}(x,y)=\\langle x,y\\rangle.\n",
    "$$\n",
    "![Explainable AI: Visualizing Attention in Transformers](./notes.assets/Screen-Shot-2023-07-11-at-5.43.47-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf3a20",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1db22c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "Используется два вектора внутреннего состояния: $h_t$ — ближайший контекст (короткая память), $c_t$ — контекст всего предложения (долгая память).\n",
    "\n",
    "Вычисляются следующие величины:\n",
    "\n",
    "- forget gate: $f = \\sigma(h_{t-1}W^f + x_t U^f+b_f)\\in(0,1)$ — регулирует, какие сигналы (компоненты $c_t$) нужно ослабить (механизм стирания из памяти)\n",
    "- input gate: $i = \\sigma(h_{t-1}W^i + x_t U^i+b_i)\\in(0,1)$ — регулирует, нужно ли добавить что-то в долговременную память\n",
    "- output gate: $o = \\sigma(h_{t-1}W^o + x_t U^o+b_o)\\in(0,1)$ — регулирует, насколько долговременную память $c_t$ уместно использовать прямо сейчас\n",
    "- активация прямо как до этого: $g=\\tanh(h_{t-1} W^g + x_t U^g+b_g)$\n",
    "\n",
    "Из них формируются $h_t$ и $c_t$:\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g,\\quad\n",
    "h_t =  o \\odot \\tanh(c_t)\n",
    "$$\n",
    "Вектор $h_t$ используется одновременно и как выход, и как внутреннее состояние.\n",
    "\n",
    "<img src=\"notes.assets/lstm.png\" style=\"zoom:67%;\" />\n",
    "\n",
    "Заметим, что в LSTM решена проблема затухания градиента (см. выражение для обновления $c_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f5009",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## GRU (gated recurrent units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e97ec1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Нет разделения на долго- и кратковременную память, используется один вектор состояния $h_t$. Вычисления:\n",
    "\n",
    "- reset gate: $r_t=\\sigma(W^rh_{t-1}+U^r x_t+b^r)\\in(0,1)$ — механизм стирания из памяти\n",
    "- update gate: $u_t=\\sigma(W^uh_{t-1}+U^ux_t+b^u)\\in(0,1)$ — механизм записи в память (комбинирует в себе input и output gate одновременно)\n",
    "- активация: $g_t=g(W^g(r_t\\odot h_{t-1})+U^gx_t+b^g)$\n",
    "- скрытое состояние: $h_t=u_t\\odot h_{t-1}+(1-u_t)\\odot g_t$\n",
    "\n",
    "<img src=\"notes.assets/gru.svg\" style=\"zoom:67%;\" />"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
